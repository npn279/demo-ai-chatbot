{
    "ML001": {
        "content": "Trong trang này:\n1. Giới thiệu\n2. Phân tích toán học\nMột số ký hiệu toán học\nHàm mất mát và bài toán tối ưu\nThuật toán tối ưu hàm mất mát\nCố định \\(\\mathbf{M} \\), tìm \\(\\mathbf{Y}\\)\nCố định \\(\\mathbf{Y} \\), tìm \\(\\mathbf{M}\\)\nTóm tắt thuật toán\n3. Ví dụ trên Python\nGiới thiệu bài toán\nHiển thị dữ liệu trên đồ thị\nCác hàm số cần thiết cho K-means clustering\nKết quả tìm được bằng thư viện scikit-learn\n4. Thảo luận\nHạn chế\nChúng ta cần biết số lượng cluster cần clustering\nNghiệm cuối cùng phụ thuộc vào các centers được khởi tạo ban đầu\nCác cluster cần có só lượng điểm gần bằng nhau\nCác cluster cần có dạng hình tròn\nKhi một cluster nằm phía trong 1 cluster khác\n5. Tài liệu tham khảo\n1. Giới thiệu\nTrong bài trước, chúng ta đã làm quen với thuật toán Linear Regression - là thuật toán đơn giản nhất trong Supervised learning. Bài này tôi sẽ giới thiệu một trong những thuật toán cơ bản nhất trong Unsupervised learning - thuật toán K-means clustering (phân cụm K-means).\nTrong thuật toán K-means clustering, chúng ta không biết nhãn (label) của từng điểm dữ liệu. Mục đích là làm thể nào để phân dữ liệu thành các cụm (cluster) khác nhau sao cho dữ liệu trong cùng một cụm có tính chất giống nhau.\nVí dụ: Một công ty muốn tạo ra những chính sách ưu đãi cho những nhóm khách hàng khác nhau dựa trên sự tương tác giữa mỗi khách hàng với công ty đó (số năm là khách hàng; số tiền khách hàng đã chi trả cho công ty; độ tuổi; giới tính; thành phố; nghề nghiệp; …). Giả sử công ty đó có rất nhiều dữ liệu của rất nhiều khách hàng nhưng chưa có cách nào chia toàn bộ khách hàng đó thành một số nhóm/cụm khác nhau. Nếu một người biết Machine Learning được đặt câu hỏi này, phương pháp đầu tiên anh (chị) ta nghĩ đến sẽ là K-means Clustering. Vì nó là một trong những thuật toán đầu tiên mà anh ấy tìm được trong các cuốn sách, khóa học về Machine Learning. Và tôi cũng chắc rằng anh ấy đã đọc blog Machine Learning cơ bản. Sau khi đã phân ra được từng nhóm, nhân viên công ty đó có thể lựa chọn ra một vài khách hàng trong mỗi nhóm để quyết định xem mỗi nhóm tương ứng với nhóm khách hàng nào. Phần việc cuối cùng này cần sự can thiệp của con người, nhưng lượng công việc đã được rút gọn đi rất nhiều.\nÝ tưởng đơn giản nhất về cluster (cụm) là tập hợp các điểm ở gần nhau trong một không gian nào đó (không gian này có thể có rất nhiều chiều trong trường hợp thông tin về một điểm dữ liệu là rất lớn). Hình bên dưới là một ví dụ về 3 cụm dữ liệu (từ giờ tôi sẽ viết gọn là cluster).\nBài toán với 3 clusters.\nGiả sử mỗi cluster có một điểm đại diện (center) màu vàng. Và những điểm xung quanh mỗi center thuộc vào cùng nhóm với center đó. Một cách đơn giản nhất, xét một điểm bất kỳ, ta xét xem điểm đó gần với center nào nhất thì nó thuộc về cùng nhóm với center đó. Tới đây, chúng ta có một bài toán thú vị: Trên một vùng biển hình vuông lớn có ba đảo hình vuông, tam giác, và tròn màu vàng như hình trên. Một điểm trên biển được gọi là thuộc lãnh hải của một đảo nếu nó nằm gần đảo này hơn so với hai đảo kia . Hãy xác định ranh giới lãnh hải của các đảo.\nHình dưới đây là một hình minh họa cho việc phân chia lãnh hải nếu có 5 đảo khác nhau được biểu diễn bằng các hình tròn màu đen:\nPhân vùng lãnh hải của mỗi đảo. Các vùng khác nhau có màu sắc khác nhau.\nChúng ta thấy rằng đường phân định giữa các lãnh hải là các đường thẳng (chính xác hơn thì chúng là các đường trung trực của các cặp điểm gần nhau). Vì vậy, lãnh hải của một đảo sẽ là một hình đa giác.\nCách phân chia này trong toán học được gọi là Voronoi Diagram.\nTrong không gian ba chiều, lấy ví dụ là các hành tinh, thì (tạm gọi là) lãnh không của mỗi hành tinh sẽ là một đa diện. Trong không gian nhiều chiều hơn, chúng ta sẽ có những thứ (mà tôi gọi là) siêu đa diện (hyperpolygon).\nQuay lại với bài toán phân nhóm và cụ thể là thuật toán K-means clustering, chúng ta cần một chút phân tích toán học trước khi đi tới phần tóm tắt thuật toán ở phần dưới. Nếu bạn không muốn đọc quá nhiều về toán, bạn có thể bỏ qua phần này. (Tốt nhất là đừng bỏ qua, bạn sẽ tiếc đấy).\n2. Phân tích toán học\nMục đích cuối cùng của thuật toán phân nhóm này là: từ dữ liệu đầu vào và số lượng nhóm chúng ta muốn tìm, hãy chỉ ra center của mỗi nhóm và phân các điểm dữ liệu vào các nhóm tương ứng. Giả sử thêm rằng mỗi điểm dữ liệu chỉ thuộc vào đúng một nhóm.\nMột số ký hiệu toán học\nGiả sử có \\(N\\) điểm dữ liệu là \\( \\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_N] \\in \\mathbb{R}^{d \\times N}\\) và \\(K < N\\) là số cluster chúng ta muốn phân chia. Chúng ta cần tìm các center \\( \\mathbf{m}_1, \\mathbf{m}_2, \\dots, \\mathbf{m}_K \\in \\mathbb{R}^{d \\times 1} \\) và label của mỗi điểm dữ liệu.\nLưu ý về ký hiệu toán học: trong các bài viết của tôi, các số vô hướng được biểu diễn bởi các chữ cái viết ở dạng không in đậm, có thể viết hoa, ví dụ \\(x_1, N, y, k\\). Các vector được biểu diễn bằng các chữ cái thường in đậm, ví dụ \\(\\mathbf{m}, \\mathbf{x}_1 \\). Các ma trận được biểu diễn bởi các chữ viết hoa in đậm, ví dụ \\(\\mathbf{X, M, Y} \\). Lưu ý này đã được nêu ở bài Linear Regression. Tôi xin được không nhắc lại trong các bài tiếp theo.\nVới mỗi điểm dữ liệu \\( \\mathbf{x}_i \\) đặt \\(\\mathbf{y}_i = [y_{i1}, y_{i2}, \\dots, y_{iK}]\\) là label vector của nó, trong đó nếu \\( \\mathbf{x}_i \\) được phân vào cluster \\(k\\) thì  \\(y_{ik} = 1\\) và \\(y_{ij} = 0, \\forall j \\neq k \\). Điều này có nghĩa là có đúng một phần tử của vector \\(\\mathbf{y}_i\\) là bằng 1 (tương ứng với cluster của \\(\\mathbf{x}_i \\)), các phần tử còn lại bằng 0. Ví dụ: nếu một điểm dữ liệu có label vector là \\([1,0,0,\\dots,0]\\) thì nó thuộc vào cluster 1, là \\([0,1,0,\\dots,0]\\) thì nó thuộc vào cluster 2, \\(\\dots\\). Cách mã hóa label của dữ liệu như thế này được gọi là biểu diễn one-hot. Chúng ta sẽ thấy cách biểu diễn one-hot này rất phổ biến trong Machine Learning ở các bài tiếp theo.\nRàng buộc của \\(\\mathbf{y}_i \\) có thể viết dưới dạng toán học như sau:\n\\[\ny_{ik} \\in \\{0, 1\\},~~~ \\sum_{k = 1}^K y_{ik} = 1 ~~~ (1)\n\\]\nHàm mất mát và bài toán tối ưu\nNếu ta coi center \\(\\mathbf{m}_k \\)  là center (hoặc representative) của mỗi cluster và ước lượng tất cả các điểm được phân vào cluster này bởi \\(\\mathbf{m}_k \\), thì một điểm dữ liệu \\(\\mathbf{x}_i \\) được phân vào cluster \\(k\\) sẽ bị sai số là \\( (\\mathbf{x}_i - \\mathbf{m}_k) \\). Chúng ta mong muốn sai số này có trị tuyệt đối nhỏ nhất nên (giống như trong bài Linear Regression) ta sẽ tìm cách để đại lượng sau đây đạt giá trị nhỏ nhất:\n\\[\n\\|\\mathbf{x}_i - \\mathbf{m}_k\\|_2^2\n\\]\nHơn nữa, vì \\(\\mathbf{x}_i \\) được phân vào cluster \\(k\\) nên \\(y_{ik} = 1, y_{ij} = 0, ~\\forall j \\neq k \\). Khi đó, biểu thức bên trên sẽ được viết lại là:\n\\[\ny_{ik}\\|\\mathbf{x}_i - \\mathbf{m}_k\\|_2^2 =  \\sum_{j=1}^K y_{ij}\\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2\n\\]\n(Hy vọng chỗ này không quá khó hiểu)\nSai số cho toàn bộ dữ liệu sẽ là:\n\\[\n\\mathcal{L}(\\mathbf{Y}, \\mathbf{M}) = \\sum_{i=1}^N\\sum_{j=1}^K y_{ij} \\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2\n\\]\nTrong đó \\( \\mathbf{Y} = [\\mathbf{y}_1; \\mathbf{y}_2; \\dots; \\mathbf{y}_N]\\), \\( \\mathbf{M} = [\\mathbf{m}_1, \\mathbf{m}_2, \\dots \\mathbf{m}_K] \\) lần lượt là các ma trận được tạo bởi label vector của mỗi điểm dữ liệu và center của mỗi cluster. Hàm số mất mát trong bài toán K-means clustering của chúng ta là hàm \\(\\mathcal{L}(\\mathbf{Y}, \\mathbf{M})\\) với ràng buộc như được nêu trong phương trình \\((1)\\).\nTóm lại, chúng ta cần tối ưu bài toán sau:\n\\[\n\\mathbf{Y}, \\mathbf{M} = \\arg\\min_{\\mathbf{Y}, \\mathbf{M}} \\sum_{i=1}^N\\sum_{j=1}^K y_{ij} \\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2~~~~~(2)\n\\]\n\\[\n\\text{subject to:} ~~ y_{ij} \\in \\{0, 1\\}~~ \\forall i, j;~~~ \\sum_{j = 1}^K y_{ij} = 1~~\\forall i\n\\]\n(subject to nghĩa là thỏa mãn điều kiện).\nNhắc lại khái niệm \\(\\arg\\min\\): Chúng ta biết ký hiệu \\(\\min\\) là giá trị nhỏ nhất của hàm số, \\(\\arg\\min\\) chính là giá trị của biến số để hàm số đó đạt giá trị nhỏ nhất đó. Nếu \\(f(x) = x^2 -2x + 1 = (x-1)^2 \\) thì giá trị nhỏ nhất của hàm số này bằng 0, đạt được khi \\(x = 1\\). Trong ví dụ này \\(\\min_{x} f(x) = 0\\) và \\(\\arg\\min_{x} f(x) = 1\\). Thêm ví dụ khác, nếu \\(x_1 = 0, x_2 = 10, x_3 = 5\\) thì ta nói \\(\\arg\\min_{i} x_i = 1\\) vì \\(1\\) là chỉ số để \\(x_i\\) đạt giá trị nhỏ nhất (bằng \\(0\\)). Biến số viết bên dưới \\(\\min\\) là biến số cúng ta cần tối ưu. Trong các bài toán tối ưu, ta thường quan tâm tới \\(\\arg\\min\\) hơn là \\(\\min\\).\nThuật toán tối ưu hàm mất mát\nBài toán \\((2)\\) là một bài toán khó tìm điểm tối ưu vì nó có thêm các điều kiện ràng buộc. Bài toán này thuộc loại mix-integer programming (điều kiện biến là số nguyên) - là loại rất khó tìm nghiệm tối ưu toàn cục (global optimal point, tức nghiệm làm cho hàm mất mát đạt giá trị nhỏ nhất có thể). Tuy nhiên, trong một số trường hợp chúng ta vẫn có thể tìm được phương pháp để tìm được nghiệm gần đúng hoặc điểm cực tiểu. (Nếu chúng ta vẫn nhớ chương trình toán ôn thi đại học thì điểm cực tiểu chưa chắc đã phải là điểm làm cho hàm số đạt giá trị nhỏ nhất).\nMột cách đơn giản để giải bài toán \\((2)\\) là xen kẽ giải \\(\\mathbf{Y}\\) và \\( \\mathbf{M}\\) khi biến còn lại được cố định. Đây là một thuật toán lặp, cũng là kỹ thuật phổ biến khi giải bài toán tối ưu. Chúng ta sẽ lần lượt giải quyết hai bài toán sau đây:\nCố định \\(\\mathbf{M} \\), tìm \\(\\mathbf{Y}\\)\nGiả sử đã tìm được các centers, hãy tìm các label vector để hàm mất mát đạt giá trị nhỏ nhất. Điều này tương đương với việc tìm cluster cho mỗi điểm dữ liệu.\nKhi các centers là cố định, bài toán tìm label vector cho toàn bộ dữ liệu có thể được chia nhỏ thành bài toán tìm label vector cho từng điểm dữ liệu \\(\\mathbf{x}_i\\) như sau:\n\\[\n\\mathbf{y}_i = \\arg\\min_{\\mathbf{y}_i} \\sum_{j=1}^K y_{ij}\\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2 ~~~ (3)\n\\]\n\\[\n\\text{subject to:} ~~ y_{ij} \\in \\{0, 1\\}~~ \\forall j;~~~ \\sum_{j = 1}^K y_{ij} = 1\n\\]\nVì chỉ có một phần tử của label vector \\(\\mathbf{y}_i\\) bằng \\(1\\) nên bài toán \\((3)\\) có thể tiếp tục được viết dưới dạng đơn giản hơn:\n\\[\nj = \\arg\\min_{j} \\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2\n\\]\nVì \\(\\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2\\) chính là bình phương khoảng cách tính từ điểm \\(\\mathbf{x}_i \\) tới center \\(\\mathbf{m}_j \\), ta có thể kết luận rằng mỗi điểm \\(\\mathbf{x}_i \\) thuộc vào cluster có center gần nó nhất! Từ đó ta có thể dễ dàng suy ra label vector của từng điểm dữ liệu.\nCố định \\(\\mathbf{Y} \\), tìm \\(\\mathbf{M}\\)\nGiả sử đã tìm được cluster cho từng điểm, hãy tìm center mới cho mỗi cluster để hàm mất mát đạt giá trị nhỏ nhất.\nMột khi chúng ta đã xác định được label vector cho từng điểm dữ liệu, bài toán tìm center cho mỗi cluster được rút gọn thành:\n\\[\n\\mathbf{m}_j = \\arg\\min_{\\mathbf{m}_j} \\sum_{i = 1}^{N} y_{ij}\\|\\mathbf{x}_i - \\mathbf{m}_j \\|_2^2.\n\\]\nTới đây, ta có thể tìm nghiệm bằng phương pháp giải đạo hàm bằng 0, vì hàm cần tối ưu là một hàm liên tục và có đạo hàm xác định tại mọi điểm. Và quan trọng hơn, hàm này là hàm convex (lồi) theo \\(\\mathbf{m}_j \\) nên chúng ta sẽ tìm được giá trị nhỏ nhất và điểm tối ưu tương ứng. Sau này nếu có dịp, tôi sẽ nói thêm về tối ưu lồi (convex optimization) - một mảng cực kỳ quan trọng trong toán tối ưu.\nĐặt \\(l(\\mathbf{m}_j)\\) là hàm bên trong dấu \\(\\arg\\min\\), ta có đạo hàm:\n\\[\n\\frac{\\partial l(\\mathbf{m}_j)}{\\partial \\mathbf{m}_j} = 2\\sum_{i=1}^N y_{ij}(\\mathbf{m}_j - \\mathbf{x}_i)\n\\]\nGiải phương trình đạo hàm bằng 0 ta có:\n\\[\n\\mathbf{m}_j \\sum_{i=1}^N y_{ij} = \\sum_{i=1}^N y_{ij} \\mathbf{x}_i\n\\]\n\\[\n\\Rightarrow \\mathbf{m}_j = \\frac{ \\sum_{i=1}^N y_{ij} \\mathbf{x}_i}{\\sum_{i=1}^N y_{ij}}\n\\]\nNếu để ý một chút, chúng ta sẽ thấy rằng mẫu số chính là phép đếm số lượng các điểm dữ liệu trong cluster \\(j\\) (Bạn có nhận ra không?). Còn tử số chính là tổng các điểm dữ liệu trong cluster \\(j\\). (Nếu bạn đọc vẫn nhớ điều kiện ràng buộc của các \\(y_{ij} \\) thì sẽ có thể nhanh chóng nhìn ra điều này).\nHay nói một cách đơn giản hơn nhiều: \\(\\mathbf{m}_j\\) là trung bình cộng của các điểm trong cluster \\(j\\).\nTên gọi K-means clustering cũng xuất phát từ đây.\nTóm tắt thuật toán\nTới đây tôi xin được tóm tắt lại thuật toán (đặc biệt quan trọng với các bạn bỏ qua phần toán học bên trên) như sau:\nĐầu vào: Dữ liệu \\(\\mathbf{X}\\) và số lượng cluster cần tìm \\(K\\).\nĐầu ra: Các center \\(\\mathbf{M}\\) và label vector cho từng điểm dữ liệu \\(\\mathbf{Y}\\).\nChọn \\(K\\) điểm bất kỳ làm các center ban đầu.\nPhân mỗi điểm dữ liệu vào cluster có center gần nó nhất.\nNếu việc gán dữ liệu vào từng cluster ở bước 2 không thay đổi so với vòng lặp trước nó thì ta dừng thuật toán.\nCập nhật center cho từng cluster bằng cách lấy trung bình cộng của tất các các điểm dữ liệu đã được gán vào cluster đó sau bước 2.\nQuay lại bước 2.\nChúng ta có thể đảm bảo rằng thuật toán sẽ dừng lại sau một số hữu hạn vòng lặp. Thật vậy, vì hàm mất mát là một số dương và sau mỗi bước 2 hoặc 3, giá trị của hàm mất mát bị giảm đi. Theo kiến thức về dãy số trong chương trình cấp 3: nếu một dãy số giảm và bị chặn dưới thì nó hội tụ! Hơn nữa, số lượng cách phân nhóm cho toàn bộ dữ liệu là hữu hạn nên đến một lúc nào đó, hàm mất mát sẽ không thể thay đổi, và chúng ta có thể dừng thuật toán tại đây.\nChúng ta sẽ có một vài thảo luận về thuật toán này, về những hạn chế và một số phương pháp khắc phục. Nhưng trước hết, hãy xem nó thể hiện như thế nào trong một ví dụ cụ thể dưới đây.\n3. Ví dụ trên Python\nGiới thiệu bài toán\nĐể kiểm tra mức độ hiểu quả của một thuật toán, chúng ta sẽ làm một ví dụ đơn giản (thường được gọi là toy example). Trước hết, chúng ta chọn center cho từng cluster và tạo dữ liệu cho từng cluster bằng cách lấy mẫu theo phân phối chuẩn có kỳ vọng là center của cluster đó và ma trận hiệp phương sai (covariance matrix) là ma trận đơn vị.\nTrước tiên, chúng ta cần khai báo các thư viện cần dùng. Chúng ta cần numpy và matplotlib như trong bài Linear Regression cho việc tính toán ma trận và hiển thị dữ liệu. Chúng ta cũng cần thêm thư viện scipy.spatial.distance để tính khoảng cách giữa các cặp điểm trong hai tập hợp một cách hiệu quả.\nfrom __future__ import print_function\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cdist\nnp.random.seed(11)\nTiếp theo, ta tạo dữ liệu bằng cách lấy các điểm theo phân phối chuẩn có kỳ vọng tại các điểm có tọa độ (2, 2), (8, 3) và (3, 6), ma trận hiệp phương sai giống nhau và là ma trận đơn vị. Mỗi cluster có 500 điểm. (Chú ý rằng mỗi điểm dữ liệu là một hàng của ma trận dữ liệu.\nmeans = [[2, 2], [8, 3], [3, 6]]\ncov = [[1, 0], [0, 1]]\nN = 500\nX0 = np.random.multivariate_normal(means[0], cov, N)\nX1 = np.random.multivariate_normal(means[1], cov, N)\nX2 = np.random.multivariate_normal(means[2], cov, N)\nX = np.concatenate((X0, X1, X2), axis = 0)\nK = 3\noriginal_label = np.asarray([0]*N + [1]*N + [2]*N).T\nHiển thị dữ liệu trên đồ thị\nChúng ta cần một hàm kmeans_display để hiển thị dữ liệu. Sau đó hiển thị dữ liệu theo nhãn ban đầu.\ndef kmeans_display(X, label):\nK = np.amax(label) + 1\nX0 = X[label == 0, :]\nX1 = X[label == 1, :]\nX2 = X[label == 2, :]\nplt.plot(X0[:, 0], X0[:, 1], 'b^', markersize = 4, alpha = .8)\nplt.plot(X1[:, 0], X1[:, 1], 'go', markersize = 4, alpha = .8)\nplt.plot(X2[:, 0], X2[:, 1], 'rs', markersize = 4, alpha = .8)\nplt.axis('equal')\nplt.plot()\nplt.show()\nkmeans_display(X, original_label)\nTrong đồ thị trên, mỗi cluster tương ứng với một màu. Có thể nhận thấy rằng có một vài điểm màu đỏ bị lẫn sang phần cluster màu xanh.\nCác hàm số cần thiết cho K-means clustering\nViết các hàm:\nkmeans_init_centers để khởi tạo các centers ban đầu.\nkmeans_asign_labels để gán nhán mới cho các điểm khi biết các centers.\nkmeans_update_centers để cập nhật các centers mới dữa trên dữ liệu vừa được gán nhãn.\nhas_converged để kiểm tra điều kiện dừng của thuật toán.\ndef kmeans_init_centers(X, k):\n# randomly pick k rows of X as initial centers\nreturn X[np.random.choice(X.shape[0], k, replace=False)]\ndef kmeans_assign_labels(X, centers):\n# calculate pairwise distances btw data and centers\nD = cdist(X, centers)\n# return index of the closest center\nreturn np.argmin(D, axis = 1)\ndef kmeans_update_centers(X, labels, K):\ncenters = np.zeros((K, X.shape[1]))\nfor k in range(K):\n# collect all points assigned to the k-th cluster\nXk = X[labels == k, :]\n# take average\ncenters[k,:] = np.mean(Xk, axis = 0)\nreturn centers\ndef has_converged(centers, new_centers):\n# return True if two sets of centers are the same\nreturn (set([tuple(a) for a in centers]) ==\nset([tuple(a) for a in new_centers]))\nPhần chính của K-means clustering:\ndef kmeans(X, K):\ncenters = [kmeans_init_centers(X, K)]\nlabels = []\nit = 0\nwhile True:\nlabels.append(kmeans_assign_labels(X, centers[-1]))\nnew_centers = kmeans_update_centers(X, labels[-1], K)\nif has_converged(centers[-1], new_centers):\nbreak\ncenters.append(new_centers)\nit += 1\nreturn (centers, labels, it)\nÁp dụng thuật toán vừa viết vào dữ liệu ban đầu, hiển thị kết quả cuối cùng.\n(centers, labels, it) = kmeans(X, K)\nprint('Centers found by our algorithm:')\nprint(centers[-1])\nkmeans_display(X, labels[-1])\nCenters found by our algorithm:\n[[ 1.97563391  2.01568065]\n[ 8.03643517  3.02468432]\n[ 2.99084705  6.04196062]]\nTừ kết quả này chúng ta thấy rằng thuật toán K-means clustering làm việc khá thành công, các centers tìm được khá gần với kỳ vọng ban đầu. Các điểm thuộc cùng một cluster hầu như được phân vào cùng một cluster (trừ một số điểm màu đỏ ban đầu đã bị phân nhầm vào cluster màu xanh da trời, nhưng tỉ lệ là nhỏ và có thể chấp nhận được).\nDưới đây là hình ảnh động minh họa thuật toán qua từng vòng lặp, chúng ta thấy rằng thuật toán trên hội tụ rất nhanh, chỉ cần 6 vòng lặp để có được kết quả cuối cùng:\nCác bạn có thể xem thêm các trang web minh họa thuật toán K-means cluster tại:\nVisualizing K-Means Clustering\nVisualizing K-Means Clustering - Standford\nKết quả tìm được bằng thư viện scikit-learn\nĐể kiểm tra thêm, chúng ta hãy so sánh kết quả trên với kết quả thu được bằng cách sử dụng thư viện scikit-learn.\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3, random_state=0).fit(X)\nprint('Centers found by scikit-learn:')\nprint(kmeans.cluster_centers_)\npred_label = kmeans.predict(X)\nkmeans_display(X, pred_label)\nCenters found by scikit-learn:\n[[ 8.0410628   3.02094748]\n[ 2.99357611  6.03605255]\n[ 1.97634981  2.01123694]]\nThật may mắn (cho tôi), hai thuật toán cho cùng một đáp số! Với cách thứ nhất, tôi mong muốn các bạn hiểu rõ được thuật toán K-means clustering làm việc như thế nào. Với cách thứ hai, tôi hy vọng các bạn biết áp dụng thư viện sẵn có như thế nào.\n4. Thảo luận\nHạn chế\nCó một vài hạn chế của thuật toán K-means clustering:\nChúng ta cần biết số lượng cluster cần clustering\nĐể ý thấy rằng trong thuật toán nêu trên, chúng ta cần biết đại lượng \\(K\\) là số lượng clusters. Trong thực tế, nhiều trường hợp chúng ta không xác định được giá trị này. Có một số phương pháp giúp xác định số lượng clusters, tôi sẽ dành thời gian nói về chúng sau nếu có dịp. Bạn đọc có thể tham khảo Elbow method - Determining the number of clusters in a data set.\nNghiệm cuối cùng phụ thuộc vào các centers được khởi tạo ban đầu\nTùy vào các center ban đầu mà thuật toán có thể có tốc độ hội tụ rất chậm, ví dụ:\nhoặc thậm chí cho chúng ta nghiệm không chính xác (chỉ là local minimum - điểm cực tiểu - mà không phải giá trị nhỏ nhất):\nCó một vài cách khắc phục đó là:\nChạy K-means clustering nhiều lần với các center ban đầu khác nhau rồi chọn cách có hàm mất mát cuối cùng đạt giá trị nhỏ nhất.\nK-means++ -Improve initialization algorithm - wiki.\nBạn nào muốn tìm hiểu sâu hơn có thể xem bài báo khoa học Cluster center initialization algorithm for K-means clustering.\nCác cluster cần có só lượng điểm gần bằng nhau\nDưới đây là một ví dụ với 3 cluster với 20, 50, và 1000 điểm. Kết quả cuối cùng không chính xác.\nCác cluster cần có dạng hình tròn\nTức các cluster tuân theo phân phối chuẩn và ma trận hiệp phương sai là ma trận đường chéo có các điểm trên đường chéo giống nhau.\nDưới đây là 1 ví dụ khi 1 cluster có dạng hình dẹt.\nKhi một cluster nằm phía trong 1 cluster khác\nĐây là ví dụ kinh điển về việc K-means clustering không thể phân cụm dữ liệu. Một cách tự nhiên, chúng ta sẽ phân ra thành 4 cụm: mắt trái, mắt phải, miệng, xung quanh mặt. Nhưng vì mắt và miệng nằm trong khuôn mặt nên K-means clustering không thực hiện được:\nMặc dù có những hạn chế, K-means clustering vẫn cực kỳ quan trọng trong Machine Learning và là nền tảng cho nhiều thuật toán phức tạp khác sau này. Chúng ta cần bắt đầu từ những thứ đơn giản. Simple is best!\n5. Tài liệu tham khảo\nClustering documents using k-means\nVoronoi Diagram - Wikipedia\nCluster center initialization algorithm for K-means clustering\nVisualizing K-Means Clustering\nVisualizing K-Means Clustering - Standford",
        "summary": "Thuật toán K-means clustering là một thuật toán phân cụm không giám sát, được sử dụng để phân chia dữ liệu thành các nhóm có tính chất giống nhau dựa trên khoảng cách đến các tâm cụm. Thuật toán này hoạt động bằng cách lặp đi lặp lại hai bước: gán nhãn cho các điểm dữ liệu dựa trên tâm cụm gần nhất và cập nhật tâm cụm dựa trên trung bình cộng của các điểm trong mỗi cụm. Mặc dù hiệu quả trong nhiều trường hợp, K-means clustering cũng có một số hạn chế, bao gồm việc cần xác định trước số lượng cụm, phụ thuộc vào tâm cụm khởi tạo ban đầu và không hiệu quả với các cụm có hình dạng không tròn hoặc khi một cụm nằm trong một cụm khác. \n",
        "status": true
    },
    "ML002": {
        "content": "Nếu như con người có kiểu học “nước đến chân mới nhảy”, thì trong Machine Learning cũng có một thuật toán như vậy.\nTrong trang này:\n1. Giới thiệu\nMột câu chuyện vui\nK-nearest neighbor\nKhoảng cách trong không gian vector\n2. Phân tích toán học\n3. Ví dụ trên Python\nBộ cơ sở dữ liệu Iris (Iris flower dataset).\nThí nghiệm\nTách training và test sets\nPhương pháp đánh giá (evaluation method)\nĐánh trọng số cho các điểm lân cận\n4. Thảo luận\nKNN cho Regression\nChuẩn hóa dữ liệu\nSử dụng các phép đo khoảng cách khác nhau\nƯu điểm của KNN\nNhược điểm của KNN\nTăng tốc cho KNN\nTry this yourself\nSource code\n5. Tài liệu tham khảo\n1. Giới thiệu\nMột câu chuyện vui\nCó một anh bạn chuẩn bị đến ngày thi cuối kỳ. Vì môn này được mở tài liệu khi thi nên anh ta không chịu ôn tập để hiểu ý nghĩa của từng bài học và mối liên hệ giữa các bài. Thay vào đó, anh thu thập tất cả các tài liệu trên lớp, bao gồm ghi chép bài giảng (lecture notes), các slides và bài tập về nhà + lời giải. Để cho chắc, anh ta ra thư viện và các quán Photocopy quanh trường mua hết tất cả các loại tài liệu liên quan (khá khen cho cậu này chịu khó tìm kiếm tài liệu). Cuối cùng, anh bạn của chúng ta thu thập được một chồng cao tài liệu để mang vào phòng thi.\nVào ngày thi, anh tự tin mang chồng tài liệu vào phòng thi. Aha, đề này ít nhất mình phải được 8 điểm. Câu 1 giống hệt bài giảng trên lớp. Câu 2 giống hệt đề thi năm ngoái mà lời giải có trong tập tài liệu mua ở quán Photocopy. Câu 3 gần giống với bài tập về nhà. Câu 4 trắc nghiệm thậm chí cậu nhớ chính xác ba tài liệu có ghi đáp án. Câu cuối cùng, 1 câu khó nhưng anh đã từng nhìn thấy, chỉ là không nhớ ở đâu thôi.\nKết quả cuối cùng, cậu ta được 4 điểm, vừa đủ điểm qua môn. Cậu làm chính xác câu 1 vì tìm được ngay trong tập ghi chú bài giảng. Câu 2 cũng tìm được đáp án nhưng lời giải của quán Photocopy sai! Câu ba thấy gần giống bài về nhà, chỉ khác mỗi một số thôi, cậu cho kết quả giống như thế luôn, vậy mà không được điểm nào. Câu 4 thì tìm được cả 3 tài liệu nhưng có hai trong đó cho đáp án A, cái còn lại cho B. Cậu chọn A và được điểm. Câu 5 thì không làm được dù còn tới 20 phút, vì tìm mãi chẳng thấy đáp án đâu - nhiều tài liệu quá cũng mệt!!\nKhông phải ngẫu nhiên mà tôi dành ra ba đoạn văn để kể về chuyện học hành của anh chàng kia. Hôm nay tôi xin trình bày về một phương pháp trong Machine Learning, được gọi là K-nearest neighbor (hay KNN), một thuật toán được xếp vào loại lazy (machine) learning (máy lười học). Thuật toán này khá giống với cách học/thi của anh bạn kém may mắn kia.\nK-nearest neighbor\nK-nearest neighbor là một trong những thuật toán supervised-learning đơn giản nhất (mà hiệu quả trong một vài trường hợp) trong Machine Learning. Khi training, thuật toán này không học một điều gì từ dữ liệu training (đây cũng là lý do thuật toán này được xếp vào loại lazy learning), mọi tính toán được thực hiện khi nó cần dự đoán kết quả của dữ liệu mới. K-nearest neighbor có thể áp dụng được vào cả hai loại của bài toán Supervised learning là Classification và Regression. KNN còn được gọi là một thuật toán Instance-based hay Memory-based learning.\nCó một vài khái niệm tương ứng người-máy như sau:\nNgôn ngữ người\nNgôn ngữ Máy Học\nin Machine Learning\nCâu hỏi\nĐiểm dữ liệu\nData point\nĐáp án\nĐầu ra, nhãn\nOutput, Label\nÔn thi\nHuấn luyện\nTraining\nTập tài liệu mang vào phòng thi\nTập dữ liệu tập huấn\nTraining set\nĐề thi\nTập dữ liểu kiểm thử\nTest set\nCâu hỏi trong dề thi\nDữ liệu kiểm thử\nTest data point\nCâu hỏi có đáp án sai\nNhiễu\nNoise, Outlier\nCâu hỏi gần giống\nĐiểm dữ liệu gần nhất\nNearest Neighbor\nVới KNN, trong bài toán Classification, label của một điểm dữ liệu mới (hay kết quả của câu hỏi trong bài thi) được suy ra trực tiếp từ K điểm dữ liệu gần nhất trong training set. Label của một test data có thể được quyết định bằng major voting (bầu chọn theo số phiếu) giữa các điểm gần nhất, hoặc nó có thể được suy ra bằng cách đánh trọng số khác nhau cho mỗi trong các điểm gần nhất đó rồi suy ra label. Chi tiết sẽ được nêu trong phần tiếp theo.\nTrong bài toán Regresssion, đầu ra của một điểm dữ liệu sẽ bằng chính đầu ra của điểm dữ liệu đã biết gần nhất (trong trường hợp K=1), hoặc là trung bình có trọng số của đầu ra của những điểm gần nhất, hoặc bằng một mối quan hệ dựa trên khoảng cách tới các điểm gần nhất đó.\nMột cách ngắn gọn, KNN là thuật toán đi tìm đầu ra của một điểm dữ liệu mới bằng cách chỉ dựa trên thông tin của K điểm dữ liệu trong training set gần nó nhất (K-lân cận), không quan tâm đến việc có một vài điểm dữ liệu trong những điểm gần nhất này là nhiễu. Hình dưới đây là một ví dụ về KNN trong classification với K = 1.\nBản đồ của 1NN (Nguồn: Wikipedia)\nVí dụ trên đây là bài toán Classification với 3 classes: Đỏ, Lam, Lục. Mỗi điểm dữ liệu mới (test data point) sẽ được gán label theo màu của điểm mà nó thuộc về. Trong hình này, có một vài vùng nhỏ xem lẫn vào các vùng lớn hơn khác màu. Ví dụ có một điểm màu Lục ở gần góc 11 giờ nằm giữa hai vùng lớn với nhiều dữ liệu màu Đỏ và Lam. Điểm này rất có thể là nhiễu. Dẫn đến nếu dữ liệu test rơi vào vùng này sẽ có nhiều khả năng cho kết quả không chính xác.\nKhoảng cách trong không gian vector\nTrong không gian một chiều, khoảng cách giữa hai điểm là trị tuyệt đối giữa hiệu giá trị của hai điểm đó. Trong không gian nhiều chiều, khoảng cách giữa hai điểm có thể được định nghĩa bằng nhiều hàm số khác nhau, trong đó độ dài đường thằng nổi hai điểm chỉ là một trường hợp đặc biệt trong đó. Nhiều thông tin bổ ích (cho Machine Learning) có thể được tìm thấy tại Norms (chuẩn) của vector trong tab Math.\n2. Phân tích toán học\nThuật toán KNN rất dễ hiểu nên sẽ phần “Phân tích toán học” này sẽ chỉ có 3 câu. Tôi trực tiếp đi vào các ví dụ. Có một điều đáng lưu ý là KNN phải nhớ tất cả các điểm dữ liệu training, việc này không được lợi về cả bộ nhớ và thời gian tính toán - giống như khi cậu bạn của chúng ta không tìm được câu trả lời cho câu hỏi cuối cùng.\n3. Ví dụ trên Python\nBộ cơ sở dữ liệu Iris (Iris flower dataset).\nIris flower dataset là một bộ dữ liệu nhỏ (nhỏ hơn rất nhiều so với MNIST. Bộ dữ liệu này bao gồm thông tin của ba loại hoa Iris (một loài hoa lan) khác nhau: Iris setosa, Iris virginica và Iris versicolor. Mỗi loại có 50 bông hoa được đo với dữ liệu là 4 thông tin: chiều dài, chiều rộng đài hoa (sepal), và chiều dài, chiều rộng cánh hoa (petal). Dưới đây là ví dụ về hình ảnh của ba loại hoa. (Chú ý, đây không phải là bộ cơ sở dữ liệu ảnh như MNIST, mỗi điểm dữ liệu trong tập này chỉ là một vector 4 chiều).\nVí dụ về Iris flower dataset (Nguồn: Wikipedia)\nBộ dữ liệu nhỏ này thường được sử dụng trong nhiều thuật toán Machine Learning trong các lớp học. Tôi sẽ giải thích lý do không chọn MNIST vào phần sau.\nThí nghiệm\nTrong phần này, chúng ta sẽ tách 150 dữ liệu trong Iris flower dataset ra thành 2 phần, gọi là training set và test set. Thuật toán KNN sẽ dựa vào trông tin ở training set để dự đoán xem mỗi dữ liệu trong test set tương ứng với loại hoa nào. Dữ liệu được dự đoán này sẽ được đối chiếu với loại hoa thật của mỗi dữ liệu trong test set để đánh giá hiệu quả của KNN.\nTrước tiên, chúng ta cần khai báo vài thư viện.\nIris flower dataset có sẵn trong thư viện scikit-learn.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import neighbors, datasets\nTiếp theo, chúng ta load dữ liệu và hiện thị vài dữ liệu mẫu. Các class được gán nhãn là 0, 1, và 2.\niris = datasets.load_iris()\niris_X = iris.data\niris_y = iris.target\nprint 'Number of classes: %d' %len(np.unique(iris_y))\nprint 'Number of data points: %d' %len(iris_y)\nX0 = iris_X[iris_y == 0,:]\nprint '\\nSamples from class 0:\\n', X0[:5,:]\nX1 = iris_X[iris_y == 1,:]\nprint '\\nSamples from class 1:\\n', X1[:5,:]\nX2 = iris_X[iris_y == 2,:]\nprint '\\nSamples from class 2:\\n', X2[:5,:]\nNumber of classes: 3\nNumber of data points: 150\nSamples from class 0:\n[[ 5.1  3.5  1.4  0.2]\n[ 4.9  3.   1.4  0.2]\n[ 4.7  3.2  1.3  0.2]\n[ 4.6  3.1  1.5  0.2]\n[ 5.   3.6  1.4  0.2]]\nSamples from class 1:\n[[ 7.   3.2  4.7  1.4]\n[ 6.4  3.2  4.5  1.5]\n[ 6.9  3.1  4.9  1.5]\n[ 5.5  2.3  4.   1.3]\n[ 6.5  2.8  4.6  1.5]]\nSamples from class 2:\n[[ 6.3  3.3  6.   2.5]\n[ 5.8  2.7  5.1  1.9]\n[ 7.1  3.   5.9  2.1]\n[ 6.3  2.9  5.6  1.8]\n[ 6.5  3.   5.8  2.2]]\nNếu nhìn vào vài dữ liệu mẫu, chúng ta thấy rằng hai cột cuối mang khá nhiều thông tin giúp chúng ta có thể  phân biệt được chúng. Chúng ta dự đoán rằng kết quả classification cho cơ sở dữ liệu này sẽ tương đối cao.\nTách training và test sets\nGiả sử chúng ta muốn dùng 50 điểm dữ liệu cho test set, 100 điểm còn lại cho training set. Scikit-learn có một hàm số cho phép chúng ta ngẫu nhiên lựa chọn các điểm này, như sau:\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\niris_X, iris_y, test_size=50)\nprint \"Training size: %d\" %len(y_train)\nprint \"Test size    : %d\" %len(y_test)\nTraining size: 100\nTest size    : 50\nSau đây, tôi trước hết xét trường hợp đơn giản K = 1, tức là với mỗi điểm test data, ta chỉ xét 1 điểm training data gần nhất và lấy label của điểm đó để dự đoán cho điểm test này.\nclf = neighbors.KNeighborsClassifier(n_neighbors = 1, p = 2)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint \"Print results for 20 test data points:\"\nprint \"Predicted labels: \", y_pred[20:40]\nprint \"Ground truth    : \", y_test[20:40]\nPrint results for first 20 test data points:\nPredicted labels:  [2 1 2 2 1 2 2 0 2 0 2 0 1 0 0 2 2 0 2 0]\nGround truth    :  [2 1 2 2 1 2 2 0 2 0 1 0 1 0 0 2 1 0 2 0]\nKết quả cho thấy label dự đoán gần giống với label thật của test data, chỉ có 2 điểm trong số 20 điểm được hiển thị có kết quả sai lệch. Ở đây chúng ta làm quen với khái niệm mới: ground truth. Một cách đơn giản, ground truth chính là nhãn/label/đầu ra thực sự của các điểm trong test data. Khái niệm này được dùng nhiều trong Machine Learning, hy vọng lần tới các bạn gặp thì sẽ nhớ ngay nó là gì.\nPhương pháp đánh giá (evaluation method)\nĐể đánh giá độ chính xác của thuật toán KNN classifier này, chúng ta xem xem có bao nhiêu điểm trong test data được dự đoán đúng. Lấy số lượng này chia cho tổng số lượng trong tập test data sẽ ra độ chính xác. Scikit-learn cung cấp hàm số accuracy_score để thực hiện công việc này.\nfrom sklearn.metrics import accuracy_score\nprint \"Accuracy of 1NN: %.2f %%\" %(100*accuracy_score(y_test, y_pred))\nAccuracy of 1NN: 94.00 %\n1NN đã cho chúng ta kết quả là 94%, không tệ! Chú ý rằng đây là một cơ sở dữ liệu dễ vì chỉ với dữ liệu ở hai cột cuối cùng, chúng ta đã có thể suy ra quy luật. Trong ví dụ này, tôi sử dụng p = 2 nghĩa là khoảng cách ở đây được tính là khoảng cách theo norm 2. Các bạn cũng có thể thử bằng cách thay p = 1 cho norm 1, hoặc các gía trị p khác cho norm khác. (Xem thêm sklearn.neighbors.KNeighborsClassifier)\nNhận thấy rằng chỉ xét 1 điểm gần nhất có thể dẫn đến kết quả sai nếu điểm đó là nhiễu. Một cách có thể làm tăng độ chính xác là tăng số lượng điểm lân cận lên, ví dụ 10 điểm, và xem xem trong 10 điểm gần nhất, class nào chiếm đa số thì dự đoán kết quả là class đó. Kỹ thuật dựa vào đa số này được gọi là major voting.\nclf = neighbors.KNeighborsClassifier(n_neighbors = 10, p = 2)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint \"Accuracy of 10NN with major voting: %.2f %%\" %(100*accuracy_score(y_test, y_pred))\nAccuracy of 10NN with major voting: 98.00 %\nKết quả đã tăng lên 98%, rất tốt!\nĐánh trọng số cho các điểm lân cận\nLà một kẻ tham lam, tôi chưa muốn dừng kết quả ở đây vì thấy rằng mình vẫn có thể cải thiện được. Trong kỹ thuật major voting bên trên, mỗi trong 10 điểm gần nhất được coi là có vai trò như nhau và giá trị lá phiếu của mỗi điểm này là như nhau. Tôi cho rằng như thế là không công bằng, vì rõ ràng rằng những điểm gần hơn nên có trọng số cao hơn (càng thân cận thì càng tin tưởng). Vậy nên tôi sẽ đánh trọng số khác nhau cho mỗi trong 10 điểm gần nhất này. Cách đánh trọng số phải thoải mãn điều kiện là một điểm càng gần điểm test data thì phải được đánh trọng số càng cao (tin tưởng hơn). Cách đơn giản nhất là lấy nghịch đảo của khoảng cách này. (Trong trường hợp test data trùng với 1 điểm dữ liệu trong training data, tức khoảng cách bằng 0, ta lấy luôn label của điểm training data).\nScikit-learn giúp chúng ta đơn giản hóa việc này bằng cách gán gía trị weights = 'distance'. (Giá trị mặc định của weights là 'uniform', tương ứng với việc coi tất cả các điểm lân cận có giá trị như nhau như ở trên).\nclf = neighbors.KNeighborsClassifier(n_neighbors = 10, p = 2, weights = 'distance')\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint \"Accuracy of 10NN (1/distance weights): %.2f %%\" %(100*accuracy_score(y_test, y_pred))\nAccuracy of 10NN (1/distance weights): 100.00 %\nAha, 100%.\nChú ý: Ngoài 2 phương pháp đánh trọng số weights = 'uniform' và weights = 'distance' ở trên, scikit-learn còn cung cấp cho chúng ta một cách để đánh trọng số một cách tùy chọn. Ví dụ, một cách đánh trọng số phổ biến khác trong Machine Learning là:\n\\[\nw_i = \\exp \\left( \\frac{-||\\mathbf{x} - \\mathbf{x}_i||_2^2}{\\sigma^2} \\right)\n\\]\ntrong đó \\(\\mathbf{x}\\) là test data, \\(\\mathbf{x}_i\\) là một điểm trong K-lân cận của \\(\\mathbf{x}\\), \\(w_i\\) là trọng số của điểm đó (ứng với điểm dữ liệu đang xét \\(\\mathbf{x}\\)), \\(\\sigma\\) là một số dương. Nhận thấy rằng hàm số này cũng thỏa mãn điều kiện: điểm càng gần \\(\\mathbf{x}\\) thì trọng số càng cao (cao nhất bằng 1). Với hàm số này, chúng ta có thể lập trình như sau:\ndef myweight(distances):\nsigma2 = .5 # we can change this number\nreturn np.exp(-distances**2/sigma2)\nclf = neighbors.KNeighborsClassifier(n_neighbors = 10, p = 2, weights = myweight)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint \"Accuracy of 10NN (customized weights): %.2f %%\" %(100*accuracy_score(y_test, y_pred))\nAccuracy of 10NN (customized weights): 98.00 %\nTrong trường hợp này, kết quả tương đương với kỹ thuật major voting. Để đánh giá chính xác hơn kết quả của KNN với K khác nhau, cách định nghĩa khoảng cách khác nhau và cách đánh trọng số khác nhau, chúng ta cần thực hiện quá trình trên với nhiều cách chia dữ liệu training và test khác nhau rồi lấy kết quả trung bình, vì rất có thể dữ liệu phân chia trong 1 trường hợp cụ thể là rất tốt hoặc rất xấu (bias). Đây cũng là cách thường được dùng khi đánh giá hiệu năng của một thuật toán cụ thể nào đó.\n4. Thảo luận\nKNN cho Regression\nVới bài toán Regression, chúng ta cũng hoàn toàn có thể sử dụng phương pháp tương tự: ước lượng đầu ra dựa trên đầu ra và khoảng cách của các điểm trong K-lân cận. Việc ước lượng như thế nào các bạn có thể tự định nghĩa tùy vào từng bài toán.\nKNN cho bài toán Regression  (Nguồn: Nearest Neighbors regression)\nChuẩn hóa dữ liệu\nKhi có một thuộc tính trong dữ liệu (hay phần tử trong vector) lớn hơn các thuộc tính khác rất nhiều (ví dụ thay vì đo bằng cm thì một kết quả lại tính bằng mm), khoảng cách giữa các điểm sẽ phụ thuộc vào thuộc tính này rất nhiều. Để có được kết quả chính xác hơn, một kỹ thuật thường được dùng là Data Normalization (chuẩn hóa dữ liệu) để đưa các thuộc tính có đơn vị đo khác nhau về cùng một khoảng giá trị, thường là từ 0 đến 1, trước khi thực hiện KNN. Có nhiều kỹ thuật chuẩn hóa khác nhau, các bạn sẽ được thấy khi tiếp tục theo dõi Blog này. Các kỹ thuật chuẩn hóa được áp dụng với không chỉ KNN mà còn với hầu hết các thuật toán khác.\nSử dụng các phép đo khoảng cách khác nhau\nNgoài norm 1 và norm 2 tôi giới thiệu trong bài này, còn rất nhiều các khoảng cách khác nhau có thể được dùng. Một ví dụ đơn giản là đếm số lượng thuộc tính khác nhau giữa hai điểm dữ liệu. Số này càng nhỏ thì hai điểm càng gần nhau. Đây chính là giả chuẩn 0 mà tôi đã giới thiệu trong Tab Math.\nƯu điểm của KNN\nĐộ phức tạp tính toán của quá trình training là bằng 0.\nViệc dự đoán kết quả của dữ liệu mới rất đơn giản.\nKhông cần giả sử gì về phân phối của các class.\nNhược điểm của KNN\nKNN rất nhạy cảm với nhiễu khi K nhỏ.\nNhư đã nói, KNN là một thuật toán mà mọi tính toán đều nằm ở khâu test. Trong đó việc tính khoảng cách tới từng điểm dữ liệu trong training set sẽ tốn rất nhiều thời gian, đặc biệt là với các cơ sở dữ liệu có số chiều lớn và có nhiều điểm dữ liệu. Với K càng lớn thì độ phức tạp cũng sẽ tăng lên. Ngoài ra, việc lưu toàn bộ dữ liệu trong bộ nhớ cũng ảnh hưởng tới hiệu năng của KNN.\nTăng tốc cho KNN\nNgoài việc tính toán khoảng cách từ một điểm test data đến tất cả các điểm trong traing set (Brute Force), có một số thuật toán khác giúp tăng tốc việc tìm kiếm này. Bạn đọc có thẻ tìm kiếm thêm với hai từ khóa: K-D Tree và Ball Tree. Tôi xin dành phần này cho độc giả tự tìm hiểu, và sẽ quay lại nếu có dịp. Chúng ta vẫn còn những thuật toán quan trọng hơn khác cần nhiều sự quan tâm hơn.\nTry this yourself\nTôi có viết một đoạn code ngắn để thực hiện việc Classification cho cơ sở dữ liệu MNIST. Các bạn hãy download toàn bộ bộ dữ liệu này về vì sau này chúng ta còn dùng nhiều, chạy thử, comment kết quả và nhận xét của các bạn vào phần comment bên dưới. Để trả lời cho câu hỏi vì sao tôi không chọn cơ sở dữ liệu này làm ví dụ, bạn đọc có thể tự tìm ra đáp án khi chạy xong đoạn code này.\nEnjoy!\n# %reset\nimport numpy as np\nfrom mnist import MNIST # require `pip install python-mnist`\n# https://pypi.python.org/pypi/python-mnist/\nimport matplotlib.pyplot as plt\nfrom sklearn import neighbors\nfrom sklearn.metrics import accuracy_score\nimport time\n# you need to download the MNIST dataset first\n# at: http://yann.lecun.com/exdb/mnist/\nmndata = MNIST('../MNIST/') # path to your MNIST folder\nmndata.load_testing()\nmndata.load_training()\nX_test = mndata.test_images\nX_train = mndata.train_images\ny_test = np.asarray(mndata.test_labels)\ny_train = np.asarray(mndata.train_labels)\nstart_time = time.time()\nclf = neighbors.KNeighborsClassifier(n_neighbors = 1, p = 2)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nend_time = time.time()\nprint \"Accuracy of 1NN for MNIST: %.2f %%\" %(100*accuracy_score(y_test, y_pred))\nprint \"Running time: %.2f (s)\" % (end_time - start_time)\nSource code\niPython Notebook cho bài này có thể download tại đây.\n5. Tài liệu tham khảo\nsklearn.neighbors.NearestNeighbors\nsklearn.model_selection.train_test_split\nTutorial To Implement k-Nearest Neighbors in Python From Scratch",
        "summary": "Thuật toán K-nearest neighbor (KNN) là một phương pháp đơn giản trong Machine Learning, hoạt động dựa trên việc tìm K điểm dữ liệu gần nhất trong tập huấn để dự đoán kết quả của dữ liệu mới, tương tự như cách học \"nước đến chân mới nhảy\" của con người. KNN có thể được áp dụng cho cả bài toán phân loại (Classification) và hồi quy (Regression), với ưu điểm là dễ hiểu và không cần giả sử gì về phân phối của dữ liệu, nhưng nhược điểm là nhạy cảm với nhiễu và tốn nhiều thời gian tính toán khi số lượng dữ liệu lớn. \nKNN thường được sử dụng trong các bài toán phân loại đơn giản, ví dụ như phân loại hoa Iris dựa trên các thông số đo được, nhưng không phù hợp với các bài toán phức tạp hơn như phân loại ảnh MNIST do tốn quá nhiều thời gian tính toán. \nĐể cải thiện hiệu năng của KNN, có thể sử dụng các kỹ thuật như đánh trọng số cho các điểm lân cận, chuẩn hóa dữ liệu, và sử dụng các thuật toán tìm kiếm nhanh hơn như K-D Tree và Ball Tree.\n",
        "status": true
    },
    "ML003": {
        "content": "Trong bài này, tôi sẽ giới thiệu một trong những thuật toán cơ bản nhất (và đơn giản nhất) của Machine Learning. Đây là một thuật toán Supervised learning có tên Linear Regression (Hồi Quy Tuyến Tính). Bài toán này đôi khi được gọi là Linear Fitting (trong thống kê) hoặc Linear Least Square.\nTrong trang này:\n1. Giới thiệu\n2. Phân tích toán học\n2.1. Dạng của Linear Regression\n2.2. Sai số dự đoán\n2.3. Hàm mất mát\n2.4. Nghiệm cho bài toán Linear Regression\n3. Ví dụ trên Python\n3.1. Bài toán\n3.2. Hiển thị dữ liệu trên đồ thị\n3.3. Nghiệm theo công thức\n3.4. Nghiệm theo thư viện scikit-learn\n4. Thảo luận\n4.1. Các bài toán có thể giải bằng Linear Regression\n4.2. Hạn chế của Linear Regression\n4.3. Các phương pháp tối ưu\n5. Tài liệu tham khảo\n1. Giới thiệu\nQuay lại ví dụ đơn giản được nêu trong bài trước: một căn nhà rộng \\(x_1 ~ \\text{m}^2\\), có \\(x_2\\) phòng ngủ và cách trung tâm thành phố \\(x_3~ \\text{km}\\) có giá là bao nhiêu. Giả sử chúng ta đã có số liệu thống kê từ 1000 căn nhà trong thành phố đó, liệu rằng khi có một căn nhà mới với các thông số về diện tích, số phòng ngủ và khoảng cách tới trung tâm, chúng ta có thể dự đoán được giá của căn nhà đó không? Nếu có thì hàm dự đoán \\(y = f(\\mathbf{x}) \\) sẽ có dạng như thế nào. Ở đây \\(\\mathbf{x} = [x_1, x_2, x_3] \\) là một vector hàng chứa thông tin input, \\(y\\) là một số vô hướng (scalar) biểu diễn output (tức giá của căn nhà trong ví dụ này).\nLưu ý về ký hiệu toán học: trong các bài viết của tôi, các số vô hướng được biểu diễn bởi các chữ cái viết ở dạng không in đậm, có thể viết hoa, ví dụ \\(x_1, N, y, k\\). Các vector được biểu diễn bằng các chữ cái thường in đậm, ví dụ \\(\\mathbf{y}, \\mathbf{x}_1 \\). Các ma trận được biểu diễn bởi các chữ viết hoa in đậm, ví dụ \\(\\mathbf{X, Y, W} \\).\nMột cách đơn giản nhất, chúng ta có thể thấy rằng: i) diện tích nhà càng lớn thì giá nhà càng cao; ii) số lượng phòng ngủ càng lớn thì giá nhà càng cao; iii) càng xa trung tâm thì giá nhà càng giảm. Một hàm số đơn giản nhất có thể mô tả mối quan hệ giữa giá nhà và 3 đại lượng đầu vào là:\n\\[y \\approx  f(\\mathbf{x}) = \\hat{y}\\]\n\\[f(\\mathbf{x}) =w_1 x_1 + w_2 x_2 + w_3 x_3 + w_0 ~~~~ (1)\\]\ntrong đó, \\(w_1, w_2, w_3, w_0\\) là các hằng số,  \\(w_0\\) còn được gọi là bias. Mối quan hệ \\(y \\approx f(\\mathbf{x})\\) bên trên là một mối quan hệ tuyến tính (linear). Bài toán chúng ta đang làm là một bài toán thuộc loại regression. Bài toán đi tìm các hệ số tối ưu \\( \\{w_1, w_2, w_3, w_0 \\}\\) chính vì vậy được gọi là bài toán Linear Regression.\nChú ý 1: \\(y\\) là giá trị thực của outcome (dựa trên số liệu thống kê chúng ta có trong tập training data), trong khi \\(\\hat{y}\\) là giá trị mà mô hình Linear Regression dự đoán được. Nhìn chung, \\(y\\) và \\(\\hat{y}\\) là hai giá trị khác nhau do có sai số mô hình, tuy nhiên, chúng ta mong muốn rằng sự khác nhau này rất nhỏ.\nChú ý 2: Linear hay tuyến tính hiểu một cách đơn giản là thẳng, phẳng. Trong không gian hai chiều, một hàm số được gọi là tuyến tính nếu đồ thị của nó có dạng một đường thẳng. Trong không gian ba chiều, một hàm số được goi là tuyến tính nếu đồ thị của nó có dạng một mặt phẳng. Trong không gian nhiều hơn 3 chiều, khái niệm mặt phẳng không còn phù hợp nữa, thay vào đó, một khái niệm khác ra đời được gọi là siêu mặt phẳng (hyperplane). Các hàm số tuyến tính là các hàm đơn giản nhất, vì chúng thuận tiện trong việc hình dung và tính toán. Chúng ta sẽ được thấy trong các bài viết sau, tuyến tính rất quan trọng và hữu ích trong các bài toán Machine Learning. Kinh nghiệm cá nhân tôi cho thấy, trước khi hiểu được các thuật toán phi tuyến (non-linear, không phẳng), chúng ta cần nắm vững các kỹ thuật cho các mô hình tuyến tính.\n2. Phân tích toán học\n2.1. Dạng của Linear Regression\nTrong phương trình \\((1)\\) phía trên, nếu chúng ta đặt \\(\\mathbf{w} = [w_0, w_1, w_2, w_3]^T = \\) là vector (cột) hệ số cần phải tối ưu và \\(\\mathbf{\\bar{x}} = [1, x_1, x_2, x_3]\\) (đọc là x bar trong tiếng Anh) là vector (hàng) dữ liệu đầu vào mở rộng. Số \\(1\\) ở đầu được thêm vào để phép tính đơn giản hơn và thuận tiện cho việc tính toán. Khi đó, phương trình (1) có thể được viết lại dưới dạng:\n\\[y \\approx \\mathbf{\\bar{x}}\\mathbf{w} = \\hat{y}\\]\nChú ý rằng \\(\\mathbf{\\bar{x}}\\) là một vector hàng. (Xem thêm về ký hiệu vector hàng và cột tại đây)\n2.2. Sai số dự đoán\nChúng ta mong muốn rằng sự sai khác \\(e\\) giữa giá trị thực \\(y\\) và giá trị dự đoán \\(\\hat{y}\\) (đọc là y hat trong tiếng Anh) là nhỏ nhất. Nói cách khác, chúng ta muốn giá trị sau đây càng nhỏ càng tốt:\n\\[\n\\frac{1}{2}e^2 = \\frac{1}{2}(y - \\hat{y})^2 = \\frac{1}{2}(y - \\mathbf{\\bar{x}}\\mathbf{w})^2\n\\]\ntrong đó hệ số \\(\\frac{1}{2} \\) (lại) là để thuận tiện cho việc tính toán (khi tính đạo hàm thì số \\(\\frac{1}{2} \\) sẽ bị triệt tiêu). Chúng ta cần \\(e^2\\) vì \\(e = y - \\hat{y} \\) có thể là một số âm, việc nói \\(e\\) nhỏ nhất sẽ không đúng vì khi \\(e = - \\infty\\) là rất nhỏ nhưng sự sai lệch là rất lớn. Bạn đọc có thể tự đặt câu hỏi: tại sao không dùng trị tuyệt đối \\( |e| \\) mà lại dùng bình phương \\(e^2\\) ở đây? Câu trả lời sẽ có ở phần sau.\n2.3. Hàm mất mát\nĐiều tương tự xảy ra với tất cả các cặp (input, outcome) \\( (\\mathbf{x}_i, y_i), i = 1, 2, \\dots, N \\), với \\(N\\) là số lượng dữ liệu quan sát được. Điều chúng ta muốn, tổng sai số là nhỏ nhất, tương đương với việc tìm \\( \\mathbf{w} \\) để hàm số sau đạt giá trị nhỏ nhất:\n\\[ \\mathcal{L}(\\mathbf{w}) = \\frac{1}{2}\\sum_{i=1}^N (y_i - \\mathbf{\\bar{x}_i}\\mathbf{w})^2 ~~~~~(2) \\]\nHàm số \\(\\mathcal{L}(\\mathbf{w}) \\) được gọi là hàm mất mát (loss function) của bài toán Linear Regression. Chúng ta luôn mong muốn rằng sự mất mát (sai số) là nhỏ nhất, điều đó đồng nghĩa với việc  tìm vector hệ số \\( \\mathbf{w} \\)  sao cho\ngiá trị của hàm mất mát này càng nhỏ càng tốt. Giá trị của \\(\\mathbf{w}\\) làm cho hàm mất mát đạt giá trị nhỏ nhất được gọi là điểm tối ưu (optimal point), ký hiệu:\n\\[ \\mathbf{w}^* = \\arg\\min_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w})  \\]\nTrước khi đi tìm lời giải, chúng ta đơn giản hóa phép toán trong phương trình hàm mất mát \\((2)\\). Đặt \\(\\mathbf{y} = [y_1; y_2; \\dots; y_N]\\) là một vector cột chứa tất cả các output của training data; \\( \\mathbf{\\bar{X}} = [\\mathbf{\\bar{x}}_1; \\mathbf{\\bar{x}}_2; \\dots; \\mathbf{\\bar{x}}_N ] \\) là ma trận dữ liệu đầu vào (mở rộng) mà mỗi hàng của nó là một điểm dữ liệu. Khi đó hàm số mất mát \\(\\mathcal{L}(\\mathbf{w})\\) được viết dưới dạng ma trận đơn giản hơn:\n\\[\n\\mathcal{L}(\\mathbf{w})\n= \\frac{1}{2}\\sum_{i=1}^N (y_i - \\mathbf{\\bar{x}}_i\\mathbf{w})^2 \\]\n\\[\n= \\frac{1}{2} \\|\\mathbf{y} - \\mathbf{\\bar{X}}\\mathbf{w} \\|_2^2 ~~~(3) \\]\nvới \\( \\| \\mathbf{z} \\|_2 \\) là Euclidean norm (chuẩn Euclid, hay khoảng cách Euclid), nói cách khác \\( \\| \\mathbf{z} \\|_2^2 \\) là tổng của bình phương mỗi phần tử của vector \\(\\mathbf{z}\\). Tới đây, ta đã có một dạng đơn giản của hàm mất mát được viết như phương trình \\((3)\\).\n2.4. Nghiệm cho bài toán Linear Regression\nCách phổ biến nhất để tìm nghiệm cho một bài toán tối ưu (chúng ta đã biết từ khi học cấp 3) là giải phương trình đạo hàm (gradient) bằng 0! Tất nhiên đó là khi việc tính đạo hàm và việc giải phương trình đạo hàm bằng 0 không quá phức tạp. Thật may mắn, với các mô hình tuyến tính, hai việc này là khả thi.\nĐạo hàm theo \\(\\mathbf{w} \\) của hàm mất mát là:\n\\[\n\\frac{\\partial{\\mathcal{L}(\\mathbf{w})}}{\\partial{\\mathbf{w}}}\n= \\mathbf{\\bar{X}}^T(\\mathbf{\\bar{X}}\\mathbf{w} - \\mathbf{y})\n\\]\nCác bạn có thể tham khảo bảng đạo hàm theo vector hoặc ma trận của một hàm số trong mục D.2 của tài liệu này. Đến đây tôi xin quay lại câu hỏi ở phần Sai số dự đoán phía trên về việc tại sao không dùng trị tuyệt đối mà lại dùng bình phương. Câu trả lời là hàm bình phương có đạo hàm tại mọi nơi, trong khi hàm trị tuyệt đối thì không (đạo hàm không xác định tại 0).\nPhương trình đạo hàm bằng 0 tương đương với:\n\\[\n\\mathbf{\\bar{X}}^T\\mathbf{\\bar{X}}\\mathbf{w} = \\mathbf{\\bar{X}}^T\\mathbf{y} \\triangleq \\mathbf{b}\n~~~ (4)\n\\]\n(ký hiệu \\(\\mathbf{\\bar{X}}^T\\mathbf{y} \\triangleq \\mathbf{b} \\) nghĩa là đặt \\(\\mathbf{\\bar{X}}^T\\mathbf{y}\\) bằng \\(\\mathbf{b}\\) ).\nNếu ma trận vuông \\( \\mathbf{A} \\triangleq \\mathbf{\\bar{X}}^T\\mathbf{\\bar{X}}\\) khả nghịch (non-singular hay invertible) thì phương trình \\((4)\\) có nghiệm duy nhất: \\( \\mathbf{w} = \\mathbf{A}^{-1}\\mathbf{b}  \\).\nVậy nếu ma trận \\(\\mathbf{A} \\) không khả nghịch (có định thức bằng 0) thì sao? Nếu các bạn vẫn nhớ các kiến thức về hệ phương trình tuyến tính, trong trường hợp này thì hoặc phương trinh \\( (4) \\) vô nghiệm, hoặc là nó có vô số nghiệm. Khi đó, chúng ta sử dụng khái niệm giả nghịch đảo \\( \\mathbf{A}^{\\dagger} \\) (đọc là A dagger trong tiếng Anh). (Giả nghịch đảo (pseudo inverse) là trường hợp tổng quát của nghịch đảo khi ma trận không khả nghịch hoặc thậm chí không vuông. Trong khuôn khổ bài viết này, tôi xin phép được lược bỏ phần này, nếu các bạn thực sự quan tâm, tôi sẽ viết một bài khác chỉ nói về giả nghịch đảo. Xem thêm: Least Squares, Pseudo-Inverses, PCA & SVD.)\nVới khái niệm giả nghịch đảo, điểm tối ưu của bài toán Linear Regression có dạng:\n\\[\n\\mathbf{w} = \\mathbf{A}^{\\dagger}\\mathbf{b} = (\\mathbf{\\bar{X}}^T\\mathbf{\\bar{X}})^{\\dagger} \\mathbf{\\bar{X}}^T\\mathbf{y}\n~~~ (5)\n\\]\n3. Ví dụ trên Python\n3.1. Bài toán\nTrong phần này, tôi sẽ chọn một ví dụ đơn giản về việc giải bài toán Linear Regression trong Python. Tôi cũng sẽ so sánh nghiệm của bài toán khi giải theo phương trình \\((5) \\) và nghiệm tìm được khi dùng thư viện scikit-learn của Python. (Đây là thư viện Machine Learning được sử dụng rộng rãi trong Python). Trong ví dụ này, dữ liệu đầu vào chỉ có 1 giá trị (1 chiều) để thuận tiện cho việc minh hoạ trong mặt phẳng.\nChúng ta có 1 bảng dữ liệu về chiều cao và cân nặng của 15 người như dưới đây:\nChiều cao (cm)\nCân nặng (kg)\nChiều cao (cm)\nCân nặng (kg)\n147\n49\n168\n60\n150\n50\n170\n72\n153\n51\n173\n63\n155\n52\n175\n64\n158\n54\n178\n66\n160\n56\n180\n67\n163\n58\n183\n68\n165\n59\nBài toán đặt ra là: liệu có thể dự đoán cân nặng của một người dựa vào chiều cao của họ không? (Trên thực tế, tất nhiên là không, vì cân nặng còn phụ thuộc vào nhiều yếu tố khác nữa, thể tích chẳng hạn). Vì blog này nói về các thuật toán Machine Learning đơn giản nên tôi sẽ giả sử rằng chúng ta có thể dự đoán được.\nChúng ta có thể thấy là cân nặng sẽ tỉ lệ thuận với chiều cao (càng cao càng nặng), nên có thể sử dụng Linear Regression model cho việc dự đoán này. Để kiểm tra độ chính xác của model tìm được, chúng ta sẽ giữ lại cột 155 và 160 cm để kiểm thử, các cột còn lại được sử dụng để huấn luyện (train) model.\n3.2. Hiển thị dữ liệu trên đồ thị\nTrước tiên, chúng ta cần có hai thư viện numpy cho đại số tuyến tính và matplotlib cho việc vẽ hình.\n# To support both python 2 and python 3\nfrom __future__ import division, print_function, unicode_literals\nimport numpy as np\nimport matplotlib.pyplot as plt\nTiếp theo, chúng ta khai báo và biểu diễn dữ liệu trên một đồ thị.\n# height (cm)\nX = np.array([[147, 150, 153, 158, 163, 165, 168, 170, 173, 175, 178, 180, 183]]).T\n# weight (kg)\ny = np.array([[ 49, 50, 51,  54, 58, 59, 60, 62, 63, 64, 66, 67, 68]]).T\n# Visualize data\nplt.plot(X, y, 'ro')\nplt.axis([140, 190, 45, 75])\nplt.xlabel('Height (cm)')\nplt.ylabel('Weight (kg)')\nplt.show()\nTừ đồ thị này ta thấy rằng dữ liệu được sắp xếp gần như theo 1 đường thẳng, vậy mô hình Linear Regression nhiều khả năng sẽ cho kết quả tốt:\n(cân nặng) = w_1*(chiều cao) + w_0\n3.3. Nghiệm theo công thức\nTiếp theo, chúng ta sẽ tính toán các hệ số w_1 và w_0 dựa vào công thức \\((5)\\). Chú ý: giả nghịch đảo của một ma trận A trong Python sẽ được tính bằng numpy.linalg.pinv(A), pinv là từ viết tắt của pseudo inverse.\n# Building Xbar\none = np.ones((X.shape[0], 1))\nXbar = np.concatenate((one, X), axis = 1)\n# Calculating weights of the fitting line\nA = np.dot(Xbar.T, Xbar)\nb = np.dot(Xbar.T, y)\nw = np.dot(np.linalg.pinv(A), b)\nprint('w = ', w)\n# Preparing the fitting line\nw_0 = w[0][0]\nw_1 = w[1][0]\nx0 = np.linspace(145, 185, 2)\ny0 = w_0 + w_1*x0\n# Drawing the fitting line\nplt.plot(X.T, y.T, 'ro')     # data\nplt.plot(x0, y0)               # the fitting line\nplt.axis([140, 190, 45, 75])\nplt.xlabel('Height (cm)')\nplt.ylabel('Weight (kg)')\nplt.show()\nw =  [[-33.73541021]\n[  0.55920496]]\nTừ đồ thị bên trên ta thấy rằng các điểm dữ liệu màu đỏ nằm khá gần đường thẳng dự đoán màu xanh. Vậy mô hình Linear Regression hoạt động tốt với tập dữ liệu training. Bây giờ, chúng ta sử dụng mô hình này để dự đoán cân nặng của hai người có chiều cao 155 và 160 cm mà chúng ta đã không dùng khi tính toán nghiệm.\ny1 = w_1*155 + w_0\ny2 = w_1*160 + w_0\nprint( u'Predict weight of person with height 155 cm: %.2f (kg), real number: 52 (kg)'  %(y1) )\nprint( u'Predict weight of person with height 160 cm: %.2f (kg), real number: 56 (kg)'  %(y2) )\nPredict weight of person with height 155 cm: 52.94 (kg), real number: 52 (kg)\nPredict weight of person with height 160 cm: 55.74 (kg), real number: 56 (kg)\nChúng ta thấy rằng kết quả dự đoán khá gần với số liệu thực tế.\n3.4. Nghiệm theo thư viện scikit-learn\nTiếp theo, chúng ta sẽ sử dụng thư viện scikit-learn của Python để tìm nghiệm.\nfrom sklearn import datasets, linear_model\n# fit the model by Linear Regression\nregr = linear_model.LinearRegression(fit_intercept=False) # fit_intercept = False for calculating the bias\nregr.fit(Xbar, y)\n# Compare two results\nprint( 'Solution found by scikit-learn  : ', regr.coef_ )\nprint( 'Solution found by (5): ', w.T)\nSolution found by scikit-learn  :  [[  -33.73541021 0.55920496]]\nSolution found by (5):  [[  -33.73541021 0.55920496 ]]\nChúng ta thấy rằng hai kết quả thu được như nhau! (Nghĩa là tôi đã không mắc lỗi nào trong cách tìm nghiệm ở phần trên)\nSource code Jupyter Notebook cho bài này.\n4. Thảo luận\n4.1. Các bài toán có thể giải bằng Linear Regression\nHàm số \\(y \\approx f(\\mathbf{x})= \\mathbf{w}^T\\mathbf{x}\\) là một hàm tuyến tính theo cả \\( \\mathbf{w}\\) và \\(\\mathbf{x}\\). Trên thực tế, Linear Regression có thể áp dụng cho các mô hình chỉ cần tuyến tính theo \\(\\mathbf{w}\\). Ví dụ:\n\\[\ny \\approx w_1 x_1 + w_2 x_2 + w_3 x_1^2 +\n\\]\n\\[\n+w_4 \\sin(x_2) + w_5 x_1x_2 + w_0\n\\]\nlà một hàm tuyến tính theo \\(\\mathbf{w}\\) và vì vậy cũng có thể được giải bằng Linear Regression. Với mỗi dữ liệu đầu vào \\(\\mathbf{x}=[x_1; x_2] \\), chúng ta tính toán dữ liệu mới \\(\\tilde{\\mathbf{x}} = [x_1, x_2, x_1^2, \\sin(x_2), x_1x_2]\\) (đọc là x tilde trong tiếng Anh) rồi áp dụng Linear Regression với dữ liệu mới này.\nXem thêm ví dụ về Quadratic Regression (Hồi Quy Bậc Hai).\n</a>\nQuadratic Regression (Nguồn:  Quadratic Regression)\n4.2. Hạn chế của Linear Regression\nHạn chế đầu tiên của Linear Regression là nó rất nhạy cảm với nhiễu (sensitive to noise). Trong ví dụ về mối quan hệ giữa chiều cao và cân nặng bên trên, nếu có chỉ\nmột cặp dữ liệu nhiễu (150 cm, 90kg) thì kết quả sẽ sai khác đi rất nhiều. Xem hình dưới đây:\nVì vậy, trước khi thực hiện Linear Regression, các nhiễu (outlier) cần\nphải được loại bỏ. Bước này được gọi là tiền xử lý (pre-processing).\nHạn chế thứ hai của Linear Regression là nó không biễu diễn được các mô hình phức tạp. Mặc dù trong phần trên, chúng ta thấy rằng phương pháp này có thể được áp dụng nếu quan hệ giữa outcome và input không nhất thiết phải là tuyến tính, nhưng mối quan hệ này vẫn đơn giản nhiều so với các mô hình thực tế. Hơn nữa, chúng ta sẽ tự hỏi: làm thế nào để xác định được các hàm \\(x_1^2, \\sin(x_2), x_1x_2\\) như ở trên?!\n4.3. Các phương pháp tối ưu\nLinear Regression là một mô hình đơn giản, lời giải cho phương trình đạo hàm bằng 0 cũng khá đơn giản. Trong hầu hết các trường hợp, chúng ta không thể giải được phương trình đạo hàm bằng 0.\nNhưng có một điều chúng ta nên nhớ, còn tính được đạo hàm là còn có hy vọng.\n5. Tài liệu tham khảo\nLinear Regression - Wikipedia\nSimple Linear Regression Tutorial for Machine Learning\nLeast Squares, Pseudo-Inverses, PCA & SVD",
        "summary": "Linear Regression là một thuật toán học có giám sát đơn giản, được sử dụng để dự đoán giá trị đầu ra dựa trên mối quan hệ tuyến tính với các giá trị đầu vào. Thuật toán này tìm kiếm các hệ số tối ưu để giảm thiểu sai số giữa giá trị dự đoán và giá trị thực tế, được thể hiện qua hàm mất mát. Linear Regression có thể được áp dụng cho các mô hình tuyến tính theo các hệ số, nhưng nó nhạy cảm với nhiễu và không thể biểu diễn các mô hình phức tạp. \n",
        "status": true
    },
    "ML004": {
        "content": "Trong trang này:1. Giới thiệuNhắc lại hai mô hình tuyến tínhMột ví dụ nhỏMô hình Logistic RegressionSigmoid function2. Hàm mất mát và phương pháp tối ưuXây dựng hàm mất mátTối ưu hàm mất mátCông thức cập nhật cho logistic sigmoid regression3. Ví dụ với PythonVí dụ với dữ liệu 1 chiềuCác hàm cần thiết cho logistic sigmoid regressionVí dụ với dữ liệu 2 chiều4. Một vài tính chất của Logistic RegressionLogistic Regression thực ra được sử dụng nhiều trong các bài toán Classification.Boundary tạo bởi Logistic Regression có dạng tuyến tính5. Thảo luận6. Tài liệu tham khảo1. Giới thiệuNhắc lại hai mô hình tuyến tínhHai mô hình tuyến tính (linear models)Linear RegressionvàPerceptron Learning Algorithm(PLA) chúng ta đã biết đều có chung một dạng:\n\\[\ny = f(\\mathbf{w}^T\\mathbf{x})\n\\]trong đó \\(f()\\) được gọi làactivation function, và \\(\\mathbf{x}\\) được hiểu là dữ liệu mở rộng với \\(x_0 = 1\\) được thêm vào để thuận tiện cho việc tính toán. Với linear regression thì \\(f(s) = s\\), với PLA thì \\(f(s) = \\text{sgn}(s)\\). Trong linear regression, tích vô hướng \\(\\mathbf{w}^T\\mathbf{x}\\) được trực tiếp sử dụng để dự đoán output \\(y\\), loại này phù hợp nếu chúng ta cần dự đoán một giá trị thực của đầu ra không bị chặn trên và dưới. Trong PLA, đầu ra chỉ nhận một trong hai giá trị \\(1\\) hoặc \\(-1 \\), phù hợp với các bài toánbinary classification.Trong bài này, tôi sẽ giới thiệu mô hình thứ ba với một activation khác, được sử dụng cho các bài toánflexiblehơn. Trong dạng này, đầu ra có thể được thể hiện dưới dạng xác suất (probability). Ví dụ: xác suất thi đỗ nếu biết thời gian ôn thi, xác suất ngày mai có mưa dựa trên những thông tin đo được trong ngày hôm nay,… Mô hình mới này của chúng ta có tên làlogistic regression. Mô hình này giống với linear regression ở khía cạnh đầu ra là số thực, và giống với PLA ở việc đầu ra bị chặn (trong đoạn \\([0, 1]\\)). Mặc dù trong tên có chứa từregression, logistic regression thường được sử dụng nhiều hơn cho các bài toán classification.Một ví dụ nhỏTôi xin được sử dụngmột ví dụ trên Wikipedia:Một nhóm 20 sinh viên dành thời gian trong khoảng từ 0 đến 6 giờ cho việc ôn thi. Thời gian ôn thi này ảnh hưởng đến xác suất sinh viên vượt qua kỳ thi như thế nào?Kết quả thu được như sau:HoursPassHoursPass.502.751.75030103.2511.2503.501.50411.7504.2511.7514.51204.7512.251512.505.51Mặc dù có một chútbất côngkhi học 3.5 giờ thì trượt, còn học 1.75 giờ thì lại đỗ, nhìn chung, học càng nhiều thì khả năng đỗ càng cao. PLA không thể áp dụng được cho bài toán này vì không thể nói một người học bao nhiêu giờ thì 100% trượt hay đỗ, và thực tế là dữ liệu này cũng khônglinearly separable(điệu kiện để PLA có thể làm việc). Chú ý rằng các điểm màu đỏ và xanh được vẽ ở hai tung độ khác nhau để tiện cho việc minh họa. Các điểm này được vẽ dùng cả dữ liệu đầu vào \\(\\mathbf{x}\\) và đầu ra \\(y). Khi ta nóilinearly seperablelà khi ta chỉ dùng dữ liệu đầu vào \\(\\mathbf{x}\\).Chúng ta biểu diễn các điểm này trên đồ thị để thấy rõ hơn:Hình 1: Ví dụ về kết quả thi dựa trên số giờ ôn tập.Nhận thấy rằng cả linear regression và PLA đều không phù hợp với bài toán này, chúng ta cần một mô hìnhflexiblehơn.Mô hình Logistic RegressionĐầu ra dự đoán của:Linear Regression: \n\\[\nf(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x}\n\\]PLA:\n\\[\nf(\\mathbf{x}) = \\text{sgn}(\\mathbf{w}^T\\mathbf{x})\n\\]Đầu ra dự đoán của logistic regression thường được viết chung dưới dạng:\n\\[\nf(\\mathbf{x}) = \\theta(\\mathbf{w}^T\\mathbf{x})\n\\]Trong đó \\(\\theta\\) được gọi là logistic function. Một số activation cho mô hình tuyến tính được cho trong hình dưới đây:Hình 2: Các activation function khác nhau.Đường màu vàng biểu diễn linear regression. Đường này không bị chặn nên không phù hợp cho bài toán này. Có mộttricknhỏ để đưa nó về dạng bị chặn:cắtphần nhỏ hơn 0 bằng cách cho chúng bằng 0,cắtcác phần lớn hơn 1 bằng cách cho chúng bằng 1. Sau đó lấy điểm trên đường thẳng này có tung độ bằng 0.5 làm điểm phân chia haiclass, đây cũng không phải là một lựa chọn tốt. Giả sử có thêm vài bạnsinh viên tiêu biểuôn tập đến 20 giờ và, tất nhiên, thi đỗ. Khi áp dụng mô hình linear regression như hình dưới đây và lấy mốc 0.5 để phân lớp, toàn bộ sinh viên thi trượt vẫn được dự đoán là trượt, nhưng rất nhiều sinh viên thi đỗ cũng được dự đoán là trượt (nếu ta coi điểm x màu xanh lục làngưỡng cứngđể đưa ra kết luận). Rõ ràng đây là một mô hình không tốt. Anh chàng sinh viên tiêu biểu này đãkéo theorất nhiều bạn khác bị trượt.Hình 3: Tại sao Linear Regression không phù hợp?Đường màu đỏ (chỉ khác với activation function của PLA ở chỗ  hai class là 0 và 1 thay vì -1 và 1) cũng thuộc dạngngưỡng cứng(hard threshold). PLA không hoạt động trong bài toán này vì dữ liệu đã cho khônglinearly separable.Các đường màu xanh lam và xanh lục phù hợp với bài toán của chúng ta hơn. Chúng có một vài tính chất quan trọng sau:Là hàm số liên tục nhận giá trị thực, bị chặn trong khoảng \\((0, 1)\\).Nếu coi điểm có tung độ là 1/2 làm điểm phân chia thì các điểm càng xa điểm này về phía bên trái có giá trị càng gần 0. Ngược lại, các điểm càng xa điểm này về phía phải có giá trị càng gần 1. Điều nàykhớpvới nhận xét rằng học càng nhiều thì xác suất đỗ càng cao và ngược lại.Mượt(smooth) nên có đạo hàm mọi nơi, có thể được lợi trong việc tối ưu.Sigmoid functionTrong số các hàm số có 3 tính chất nói trên thì hàmsigmoid:\n\\[\nf(s) = \\frac{1}{1 + e^{-s}} \\triangleq \\sigma(s)\n\\]\nđược sử dụng nhiều nhất, vì nó bị chặn trong khoảng \\((0, 1)\\). Thêm nữa:\n\\[\n\\lim_{s \\rightarrow -\\infty}\\sigma(s) = 0; ~~ \\lim_{s \\rightarrow +\\infty}\\sigma(s) = 1 \n\\]\nĐặc biệt hơn nữa:\n\\[\n\\begin{eqnarray}\n\\sigma’(s) &=& \\frac{e^{-s}}{(1 + e^{-s})^2} \\newline\n&=& \\frac{1}{1 + e^{-s}} \\frac{e^{-s}}{1 + e^{-s}} \\newline\n&=& \\sigma(s)(1 - \\sigma(s))\n\\end{eqnarray}\n\\]\nCông thức đạo hàm đơn giản thế này giúp hàm số này được sử dụng rộng rãi. Ở phần sau, tôi sẽ lý giải việcngười ta đã tìm ra hàm số đặc biệt này như thế nào.Ngoài ra, hàmtanhcũng hay được sử dụng: \n\\[\n\\text{tanh}(s) = \\frac{e^{s} - e^{-s}}{e^s + e^{-s}}\n\\]Hàm số này nhận giá trị trong khoảng \\((-1, 1)\\) nhưng có thể dễ dàng đưa nó về khoảng \\((0, 1)\\). Bạn đọc có thể chứng minh được:\n\\[\n\\text{tanh}(s) = 2\\sigma(2s) - 1\n\\]2. Hàm mất mát và phương pháp tối ưuXây dựng hàm mất mátVới mô hình như trên (các activation màu xanh lam và lục), ta có thể giả sử rằng xác suất để một điểm dữ liệu \\(\\mathbf{x}\\) rơi vào class 1 là \\(f(\\mathbf{w}^T\\mathbf{x})\\) và rơi vào class 0 là \\(1 - f(\\mathbf{w}^T\\mathbf{x})\\). Với mô hình được giả sử như vậy, với các điểm dữ liệu training (đã biết đầu ra \\(y\\)), ta có thể viết như sau:\\[\n\\begin{eqnarray}\nP(y_i = 1 | \\mathbf{x}_i; \\mathbf{w}) &=& &f(\\mathbf{w}^T\\mathbf{x}_i)  ~~(1) \\newline\nP(y_i = 0 | \\mathbf{x}_i; \\mathbf{w}) &=& 1 - &f(\\mathbf{w}^T\\mathbf{x}_i)  ~~(2) \\newline\n\\end{eqnarray}\n\\]\ntrong đó \\( P(y_i = 1 | \\mathbf{x}_i; \\mathbf{w})\\) được hiểu là xác suất xảy ra sự kiện đầu ra \\(y_i = 1\\) khi biết tham số mô hình \\(\\mathbf{w}\\) và dữ liệu đầu vào \\(\\mathbf{x}_i\\). Bạn đọc có thể đọc thêmXác suất có điều kiện. Mục đích của chúng ta là tìm các hệ số \\(\\mathbf{w}\\) sao cho \\(f(\\mathbf{w}^T\\mathbf{x}_i)\\) càng gần với 1 càng tốt với các điểm dữ liệu thuộc class 1 và càng gần với 0 càng tốt với những điểm thuộc class 0.Ký hiệu \\(z_i = f(\\mathbf{w}^T\\mathbf{x}_i)\\) và viết gộp lại hai biểu thức bên trên ta có:\n\\[\nP(y_i| \\mathbf{x}_i; \\mathbf{w}) = z_i^{y_i}(1 - z_i)^{1- y_i}\n\\]Biểu thức này là tương đương với hai biểu thức \\((1)\\) và \\((2)\\) ở trên vì khi \\(y_i=1\\), phần thứ hai của vế phải sẽ triệt tiêu, khi \\(y_i = 0\\), phần thứ nhất sẽ bị triệt tiêu! Chúng ta muốn mô hình gần với dữ liệu đã cho nhất, tức xác suất này đạt giá trị cao nhất.Xét toàn bộ training set với \\(\\mathbf{X} = [\\mathbf{x}_1,\\mathbf{x}_2, \\dots, \\mathbf{x}_N] \\in \\mathbb{R}^{d \\times N}\\) và \\(\\mathbf{y} = [y_1, y_2, \\dots, y_N]\\), chúng ta cần tìm \\(\\mathbf{w}\\) để biểu thức sau đây đạt giá trị lớn nhất:\n\\[\nP(\\mathbf{y}|\\mathbf{X}; \\mathbf{w})\n\\]\nở đây, ta cũng ký hiệu \\(\\mathbf{X, y}\\) như cácbiến ngẫu nhiên(random variables). Nói cách khác:\n\\[\n\\mathbf{w} = \\arg\\max_{\\mathbf{w}} P(\\mathbf{y}|\\mathbf{X}; \\mathbf{w})\n\\]Bài toán tìm tham số để mô hình gần với dữ liệu nhất trên đây có tên gọi chung là bài toánmaximum likelihood estimationvới hàm số phía sau \\(\\arg\\max\\) được gọi làlikelihood function. Khi làm việc với các bài toán Machine Learning sử dụng các mô hình xác suất thống kê, chúng ta sẽ gặp lại các bài toán thuộc dạng này, hoặcmaximum a posteriori estimation, rất nhiều. Tôi sẽ dành 1 bài khác để nói về hai dạng bài toán này.Giả sử thêm rằng các điểm dữ liệu được sinh ra một cách ngẫu nhiên độc lập với nhau (independent), ta có thể viết:\n\\[\n\\begin{eqnarray}\nP(\\mathbf{y}|\\mathbf{X}; \\mathbf{w}) &=& \\prod_{i=1}^N P(y_i| \\mathbf{x}_i; \\mathbf{w}) \\newline\n&=& \\prod_{i=1}^N z_i^{y_i}(1 - z_i)^{1- y_i}\n\\end{eqnarray}\n\\]\nvới \\(\\prod\\) là ký hiệu của tích. Bạn đọc có thể muốn đọc thêm vềĐộc lập thống kê.Trực tiếp tối ưu hàm số này theo \\(\\mathbf{w}\\) nhìn qua không đơn giản! Hơn nữa, khi \\(N\\) lớn, tích của \\(N\\) số nhỏ hơn 1 có thể dẫn tới sai số trong tính toán (numerial error) vì tích là một số quá nhỏ. Một phương pháp thường được sử dụng đó là lấy logarit tự nhiên (cơ số \\(e\\)) củalikelihood functionbiến phép nhân thành phép cộng và để tránh việc số quá nhỏ. Sau đó lấy ngược dấu để được một hàm và coi nó là hàm mất mát. Lúc này bài toán tìm giá trị lớn nhất (maximum likelihood) trở thành bài toán tìm giá trị nhỏ nhất của hàm mất mát (hàm này còn được gọi là negative log likelihood):\n\\[\n\\begin{eqnarray}\nJ(\\mathbf{w}) = -\\log P(\\mathbf{y}|\\mathbf{X}; \\mathbf{w}) \\newline\n= -\\sum_{i=1}^N(y_i \\log {z}_i + (1-y_i) \\log (1 - {z}_i))\n\\end{eqnarray}\n\\]\nvới chú ý rằng \\(z_i\\) là một hàm số của \\(\\mathbf{w}\\). Bạn đọc tạm nhớ biểu thức vế phải có tên gọi làcross entropy, thường được sử dụng để đokhoảng cáchgiữa hai phân phối (distributions). Trong bài toán đang xét, một phân phối là dữ liệu được cho, với xác suất chỉ là 0 hoặc 1; phân phối còn lại được tính theo mô hình logistic regression.Khoảng cáchgiữa hai phân phối nhỏ đồng nghĩa với việc (có vẻ hiển nhiên là) hai phân phối đó rất gần nhau. Tính chất cụ thể của hàm số này sẽ được đề cập trong một bài khác mà tầm quan trọng của khoảng cách giữa hai phân phối là lớn hơn.Chú ý:Trong machine learning, logarit thập phân ít được dùng, vì vậy \\(\\log\\) thường được dùng để ký hiệu logarit tự nhiên.Tối ưu hàm mất mátChúng ta lại sử dụng phương phápStochastic Gradient Descent(SGD) ở đây (Bạn đọc được khuyến khích đọc SGD trước khi đọc phần này) . Hàm mất mát với chỉ một điểm dữ liệu \\((\\mathbf{x}_i, y_i)\\) là:\n\\[\nJ(\\mathbf{w}; \\mathbf{x}_i, y_i) = -(y_i \\log {z}_i + (1-y_i) \\log (1 - {z}_i))\n\\]Với đạo hàm:\n\\[\n\\begin{eqnarray}\n\\frac{\\partial J(\\mathbf{w}; \\mathbf{x}_i, y_i)}{\\partial \\mathbf{w}} &=& -(\\frac{y_i}{z_i} - \\frac{1- y_i}{1 - z_i} ) \\frac{\\partial z_i}{\\partial \\mathbf{w}} \\newline\n&=& \\frac{z_i - y_i}{z_i(1 - z_i)} \\frac{\\partial z_i}{\\partial \\mathbf{w}} ~~~~~~ (3)\n\\end{eqnarray}\n\\]Để cho biểu thức này trở nêngọnvàđẹphơn, chúng ta sẽ tìm hàm \\(z = f(\\mathbf{w}^T\\mathbf{x})\\) sao cho mẫu số bị triệt tiêu. Nếu đặt \\(s = \\mathbf{w}^T\\mathbf{x}\\), chúng ta sẽ có:\n\\[\n\\frac{\\partial z_i}{\\partial \\mathbf{w}} = \\frac{\\partial z_i}{\\partial s} \\frac{\\partial s}{\\partial \\mathbf{w}} = \\frac{\\partial z_i}{\\partial s} \\mathbf{x}\n\\]\nMột cách trực quan nhất, ta sẽ tìm hàm số \\(z = f(s)\\) sao cho:\n\\[\n\\frac{\\partial z}{\\partial s} = z(1 - z) ~~ (4)\n\\]\nđể triệt tiêu mẫu số trong biểu thức \\((3)\\). Chúng ta cùng khởi động một chút với phương trình vi phân đơn giản này. Phương trình \\((4)\\) tương đương với:\n\\[\n\\begin{eqnarray}\n&\\frac{\\partial z}{z(1-z)} &=& \\partial s \\newline\n\\Leftrightarrow & (\\frac{1}{z} + \\frac{1}{1 - z})\\partial z &=&\\partial s \\newline\n\\Leftrightarrow & \\log z - \\log(1 - z) &=& s \\newline\n\\Leftrightarrow & \\log \\frac{z}{1 - z} &=& s \\newline\n\\Leftrightarrow & \\frac{z}{1 - z} &=& e^s \\newline\n\\Leftrightarrow & z &=& e^s (1 - z) \\newline\n\\Leftrightarrow & z = \\frac{e^s}{1 +e^s} &=&\\frac{1}{1 + e^{-s}} = \\sigma(s)\n\\end{eqnarray}\n\\]\nĐến đây, tôi hy vọng các bạn đã hiểu hàm sốsigmoidđược tạo ra như thế nào.Chú ý: Trong việc giải phương trình vi phân ở trên, tôi đã bỏ qua hằng số khi lấy nguyên hàm hai vế. Tuy vậy, việc này không ảnh hưởng nhiều tới kết quả.Công thức cập nhật cho logistic sigmoid regressionTới đây, bạn đọc có thể kiểm tra rằng:\n\\[\n\\frac{\\partial J(\\mathbf{w}; \\mathbf{x}_i, y_i)}{\\partial \\mathbf{w}} = (z_i - y_i)\\mathbf{x}_i\n\\]\nQúa đẹp!Và công thức cập nhật (theo thuật toánSGD) cho logistic regression là: \n\\[\n\\mathbf{w} = \\mathbf{w} + \\eta(y_i - z_i)\\mathbf{x}_i\n\\]\nKhá đơn giản! Và, như thường lệ, chúng ta sẽ có vài ví dụ với Python.3. Ví dụ với PythonVí dụ với dữ liệu 1 chiềuQuay trở lại với ví dụ nêu ở phần Giới thiệu. Trước tiên ta cần khai báo vài thư viện và dữ liệu:# To support both python 2 and python 3from__future__importdivision,print_function,unicode_literalsimportnumpyasnpimportmatplotlib.pyplotaspltnp.random.seed(2)X=np.array([[0.50,0.75,1.00,1.25,1.50,1.75,1.75,2.00,2.25,2.50,2.75,3.00,3.25,3.50,4.00,4.25,4.50,4.75,5.00,5.50]])y=np.array([0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1])# extended dataX=np.concatenate((np.ones((1,X.shape[1])),X),axis=0)Các hàm cần thiết cho logistic sigmoid regressiondefsigmoid(s):return1/(1+np.exp(-s))deflogistic_sigmoid_regression(X,y,w_init,eta,tol=1e-4,max_count=10000):w=[w_init]it=0N=X.shape[1]d=X.shape[0]count=0check_w_after=20whilecount<max_count:# mix datamix_id=np.random.permutation(N)foriinmix_id:xi=X[:,i].reshape(d,1)yi=y[i]zi=sigmoid(np.dot(w[-1].T,xi))w_new=w[-1]+eta*(yi-zi)*xicount+=1# stopping criteriaifcount%check_w_after==0:ifnp.linalg.norm(w_new-w[-check_w_after])<tol:returnww.append(w_new)returnweta=.05d=X.shape[0]w_init=np.random.randn(d,1)w=logistic_sigmoid_regression(X,y,w_init,eta)print(w[-1])[[-4.092695  ]\n [ 1.55277242]]Với kết quả tìm được, đầu ra \\(y\\) có thể được dự đoán theo công thức:y = sigmoid(-4.1 + 1.55*x). Với dữ liệu trong tập training, kết quả là:print(sigmoid(np.dot(w[-1].T,X)))[[ 0.03281144  0.04694533  0.06674738  0.09407764  0.13102736  0.17961209\n   0.17961209  0.24121129  0.31580406  0.40126557  0.49318368  0.58556493\n   0.67229611  0.74866712  0.86263755  0.90117058  0.92977426  0.95055357\n   0.96541314  0.98329067]]Biểu diễn kết quả này trên đồ thị ta có:X0=X[1,np.where(y==0)][0]y0=y[np.where(y==0)]X1=X[1,np.where(y==1)][0]y1=y[np.where(y==1)]plt.plot(X0,y0,'ro',markersize=8)plt.plot(X1,y1,'bs',markersize=8)xx=np.linspace(0,6,1000)w0=w[-1][0][0]w1=w[-1][1][0]threshold=-w0/w1yy=sigmoid(w0+w1*xx)plt.axis([-2,8,-1,2])plt.plot(xx,yy,'g-',linewidth=2)plt.plot(threshold,.5,'y^',markersize=8)plt.xlabel('studying hours')plt.ylabel('predicted probability of pass')plt.show()Hình 4: Dữ liệu và hàm sigmoid tìm được.Nếu như chỉ có hai output là ‘fail’ hoặc ‘pass’, điểm trên đồ thị của hàm sigmoid tương ứng với xác suất 0.5 được chọn làmhard threshold(ngưỡng cứng). Việc này có thể chứng minh khá dễ dàng (tôi sẽ bàn ở phần dưới).Ví dụ với dữ liệu 2 chiềuChúng ta xét thêm một ví dụ nhỏ nữa trong không gian hai chiều. Giả sử chúng ta có hai class xanh-đỏ với dữ liệu được phân bố như hình dưới.Hình 5: Hai class với dữ liệu hai chiều.Với dữ liệu đầu vào nằm trong không gian hai chiều, hàm sigmoid có dạng như thác nước dưới đây:Hình 6: Hàm sigmoid với dữ liệu có chiều là 2. (Nguồn:Biased and non biased neurons)Kết quả tìm được khi áp dụng mô hình logistic regression được minh họa như hình dưới với màu nền khác nhau thể hiện xác suất điểm đó thuộc class đỏ. Đỏ hơn tức gần 1 hơn, xanh hơn tức gần 0 hơn.Hình 7: Logistic Regression với dữ liệu hai chiều.Nếu phải lựa chọn mộtngưỡng cứng(chứ không chấp nhận xác suất) để phân chia hai class, chúng ta quan sát thấy đường thẳng nằm nằm trong khu vực xanh lục là một lựa chọn hợp lý. Tôi sẽ chứng minh ở phần dưới rằng, đường phân chia giữa hai class tìm được bởi logistic regression có dạng một đường phẳng, tức vẫn là linear.4. Một vài tính chất của Logistic RegressionLogistic Regression thực ra được sử dụng nhiều trong các bài toán Classification.Mặc dù có tên là Regression, tức một mô hình cho fitting, Logistic Regression lại được sử dụng nhiều trong các bài toán Classification. Sau khi tìm được mô hình, việc xác định class \\(y\\) cho một điểm dữ liệu \\(\\mathbf{x}\\) được xác định bằng việc so sánh hai biểu thức xác suất:\n\\[\nP(y = 1| \\mathbf{x}; \\mathbf{w}); ~~ P(y = 0| \\mathbf{x}; \\mathbf{w}) \n\\]\nNếu biểu thức thứ nhất lớn hơn thì ta kết luận điểm dữ liệu thuộc class 1, ngược lại thì nó thuộc class 0. Vì tổng hai biểu thức này luôn bằng 1 nên một cách gọn hơn, ta chỉ cần xác định xem \\(P(y = 1| \\mathbf{x}; \\mathbf{w})\\) lớn hơn 0.5 hay không. Nếu có, class 1. Nếu không, class 0.Boundary tạo bởi Logistic Regression có dạng tuyến tínhThật vậy, theo lập luận ở phần trên thì chúng ta cần kiểm tra:\\[\n\\begin{eqnarray}\nP(y = 1| \\mathbf{x}; \\mathbf{w}) &>& 0.5 \\newline\n\\Leftrightarrow \\frac{1}{1 + e^{-\\mathbf{w}^T\\mathbf{x}}} &>& 0.5 \\newline\n\\Leftrightarrow e^{-\\mathbf{w}^T\\mathbf{x}} &<& 1 \\newline\n\\Leftrightarrow \\mathbf{w}^T\\mathbf{x} &>& 0\n\\end{eqnarray}\n\\]Nói cách khác, boundary giữa hai class là đường có phương trình \\(\\mathbf{w}^T\\mathbf{x}\\). Đây chính là phương trình của một siêu mặt phẳng. Vậy Logistic Regression tạo ra boundary có dạng tuyến tính.5. Thảo luậnMột điểm cộng cho Logistic Regression so với PLA là nó không cần có giả thiết dữ liệu hai class là linearly separable. Tuy nhiên, boundary tìm được vẫn có dạng tuyến tính. Vậy nên mô hình này chỉ phù hợp với loại dữ liệu mà hai class là gần với linearly separable. Một kiểu dữ liệu mà Logistic Regression không làm việc được là dữ liệu mà\nmột class chứa các điểm nằm trong 1 vòng tròn, class kia chứa các điểm bên ngoài đường tròn đó. Kiểu dữ liệu này được gọi là phi tuyến (non-linear). Sau một vài bài nữa, tôi sẽ giới thiệu với các bạn các mô hình khác phù hợp hơn với loại dữ liệu này hơn.Một hạn chế nữa của Logistic Regression là nó yêu cầu các điểm dữ liệu được tạo ra một cáchđộc lậpvới nhau. Trên thực tế, các điểm dữ liệu có thể bịảnh hưởngbởi nhau. Ví dụ: có một nhóm ôn tập với nhau trong 4 giờ, cả nhóm đều thi đỗ (giả sử các bạn này học rất tập trung), nhưng có một sinh viên học một mình cũng trong 4 giờ thì xác suất thi đỗ thấp hơn. Mặc dù vậy, để cho đơn giản, khi xây dựng mô hình, người ta vẫn thường giả sử các điểm dữ liệu là độc lập với nhau.Khi biểu diễn theo Neural Networks, Linear Regression, PLA, và Logistic Regression có dạng như sau:Hình 8: Biểu diễn Linear Regression, PLA, và Logistic Regression theo Neural network.Nếu hàm mất mát của Logistic Regression được viết dưới dạng:\n\\[\nJ(\\mathbf{w}) = \\sum_{i=1}^N (y_i - z_i)^2\n\\]\nthì khó khăn gì sẽ xảy ra? Các bạn hãy coi đây như một bài tập nhỏ.Source code cho các ví dụ trong bài này có thểtìm thấy ở đây.6. Tài liệu tham khảo[1] Cox, David R. “The regression analysis of binary sequences.” Journal of the Royal Statistical Society. Series B (Methodological) (1958): 215-242.[2] Cramer, Jan Salomon. “The origins of logistic regression.” (2002).[3] Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin. Learning from data. Vol. 4. New York, NY, USA:: AMLBook, 2012. (link to course)[4] Bishop, Christopher M. “Pattern recognition and Machine Learning.”, Springer  (2006). (book)[5] Duda, Richard O., Peter E. Hart, and David G. Stork. Pattern classification. John Wiley & Sons, 2012.[6] Andrer Ng. CS229 Lecture notes.Part II: Classification and logistic regression[7] Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie.The Elements of Statistical Learning.",
        "summary": "Mô hình Logistic Regression là một mô hình tuyến tính được sử dụng rộng rãi trong các bài toán phân loại, với đầu ra dự đoán là xác suất thuộc một lớp nhất định, thường được biểu diễn bằng hàm sigmoid. Hàm mất mát của mô hình được tối ưu hóa bằng phương pháp Stochastic Gradient Descent (SGD), và công thức cập nhật cho các tham số mô hình được rút gọn một cách đơn giản và hiệu quả. Logistic Regression tạo ra ranh giới phân loại có dạng tuyến tính, phù hợp với các dữ liệu mà hai lớp gần với linearly separable, nhưng không phù hợp với dữ liệu phi tuyến. \n",
        "status": true
    },
    "ML005": {
        "content": "Trong trang này:\n1. Giới thiệu\n2. Phân tích toán học\nMột số ký hiệu toán học\nHàm mất mát và bài toán tối ưu\nThuật toán tối ưu hàm mất mát\nCố định \\(\\mathbf{M} \\), tìm \\(\\mathbf{Y}\\)\nCố định \\(\\mathbf{Y} \\), tìm \\(\\mathbf{M}\\)\nTóm tắt thuật toán\n3. Ví dụ trên Python\nGiới thiệu bài toán\nHiển thị dữ liệu trên đồ thị\nCác hàm số cần thiết cho K-means clustering\nKết quả tìm được bằng thư viện scikit-learn\n4. Thảo luận\nHạn chế\nChúng ta cần biết số lượng cluster cần clustering\nNghiệm cuối cùng phụ thuộc vào các centers được khởi tạo ban đầu\nCác cluster cần có só lượng điểm gần bằng nhau\nCác cluster cần có dạng hình tròn\nKhi một cluster nằm phía trong 1 cluster khác\n5. Tài liệu tham khảo\n1. Giới thiệu\nTrong bài trước, chúng ta đã làm quen với thuật toán Linear Regression - là thuật toán đơn giản nhất trong Supervised learning. Bài này tôi sẽ giới thiệu một trong những thuật toán cơ bản nhất trong Unsupervised learning - thuật toán K-means clustering (phân cụm K-means).\nTrong thuật toán K-means clustering, chúng ta không biết nhãn (label) của từng điểm dữ liệu. Mục đích là làm thể nào để phân dữ liệu thành các cụm (cluster) khác nhau sao cho dữ liệu trong cùng một cụm có tính chất giống nhau.\nVí dụ: Một công ty muốn tạo ra những chính sách ưu đãi cho những nhóm khách hàng khác nhau dựa trên sự tương tác giữa mỗi khách hàng với công ty đó (số năm là khách hàng; số tiền khách hàng đã chi trả cho công ty; độ tuổi; giới tính; thành phố; nghề nghiệp; …). Giả sử công ty đó có rất nhiều dữ liệu của rất nhiều khách hàng nhưng chưa có cách nào chia toàn bộ khách hàng đó thành một số nhóm/cụm khác nhau. Nếu một người biết Machine Learning được đặt câu hỏi này, phương pháp đầu tiên anh (chị) ta nghĩ đến sẽ là K-means Clustering. Vì nó là một trong những thuật toán đầu tiên mà anh ấy tìm được trong các cuốn sách, khóa học về Machine Learning. Và tôi cũng chắc rằng anh ấy đã đọc blog Machine Learning cơ bản. Sau khi đã phân ra được từng nhóm, nhân viên công ty đó có thể lựa chọn ra một vài khách hàng trong mỗi nhóm để quyết định xem mỗi nhóm tương ứng với nhóm khách hàng nào. Phần việc cuối cùng này cần sự can thiệp của con người, nhưng lượng công việc đã được rút gọn đi rất nhiều.\nÝ tưởng đơn giản nhất về cluster (cụm) là tập hợp các điểm ở gần nhau trong một không gian nào đó (không gian này có thể có rất nhiều chiều trong trường hợp thông tin về một điểm dữ liệu là rất lớn). Hình bên dưới là một ví dụ về 3 cụm dữ liệu (từ giờ tôi sẽ viết gọn là cluster).\nBài toán với 3 clusters.\nGiả sử mỗi cluster có một điểm đại diện (center) màu vàng. Và những điểm xung quanh mỗi center thuộc vào cùng nhóm với center đó. Một cách đơn giản nhất, xét một điểm bất kỳ, ta xét xem điểm đó gần với center nào nhất thì nó thuộc về cùng nhóm với center đó. Tới đây, chúng ta có một bài toán thú vị: Trên một vùng biển hình vuông lớn có ba đảo hình vuông, tam giác, và tròn màu vàng như hình trên. Một điểm trên biển được gọi là thuộc lãnh hải của một đảo nếu nó nằm gần đảo này hơn so với hai đảo kia . Hãy xác định ranh giới lãnh hải của các đảo.\nHình dưới đây là một hình minh họa cho việc phân chia lãnh hải nếu có 5 đảo khác nhau được biểu diễn bằng các hình tròn màu đen:\nPhân vùng lãnh hải của mỗi đảo. Các vùng khác nhau có màu sắc khác nhau.\nChúng ta thấy rằng đường phân định giữa các lãnh hải là các đường thẳng (chính xác hơn thì chúng là các đường trung trực của các cặp điểm gần nhau). Vì vậy, lãnh hải của một đảo sẽ là một hình đa giác.\nCách phân chia này trong toán học được gọi là Voronoi Diagram.\nTrong không gian ba chiều, lấy ví dụ là các hành tinh, thì (tạm gọi là) lãnh không của mỗi hành tinh sẽ là một đa diện. Trong không gian nhiều chiều hơn, chúng ta sẽ có những thứ (mà tôi gọi là) siêu đa diện (hyperpolygon).\nQuay lại với bài toán phân nhóm và cụ thể là thuật toán K-means clustering, chúng ta cần một chút phân tích toán học trước khi đi tới phần tóm tắt thuật toán ở phần dưới. Nếu bạn không muốn đọc quá nhiều về toán, bạn có thể bỏ qua phần này. (Tốt nhất là đừng bỏ qua, bạn sẽ tiếc đấy).\n2. Phân tích toán học\nMục đích cuối cùng của thuật toán phân nhóm này là: từ dữ liệu đầu vào và số lượng nhóm chúng ta muốn tìm, hãy chỉ ra center của mỗi nhóm và phân các điểm dữ liệu vào các nhóm tương ứng. Giả sử thêm rằng mỗi điểm dữ liệu chỉ thuộc vào đúng một nhóm.\nMột số ký hiệu toán học\nGiả sử có \\(N\\) điểm dữ liệu là \\( \\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_N] \\in \\mathbb{R}^{d \\times N}\\) và \\(K < N\\) là số cluster chúng ta muốn phân chia. Chúng ta cần tìm các center \\( \\mathbf{m}_1, \\mathbf{m}_2, \\dots, \\mathbf{m}_K \\in \\mathbb{R}^{d \\times 1} \\) và label của mỗi điểm dữ liệu.\nLưu ý về ký hiệu toán học: trong các bài viết của tôi, các số vô hướng được biểu diễn bởi các chữ cái viết ở dạng không in đậm, có thể viết hoa, ví dụ \\(x_1, N, y, k\\). Các vector được biểu diễn bằng các chữ cái thường in đậm, ví dụ \\(\\mathbf{m}, \\mathbf{x}_1 \\). Các ma trận được biểu diễn bởi các chữ viết hoa in đậm, ví dụ \\(\\mathbf{X, M, Y} \\). Lưu ý này đã được nêu ở bài Linear Regression. Tôi xin được không nhắc lại trong các bài tiếp theo.\nVới mỗi điểm dữ liệu \\( \\mathbf{x}_i \\) đặt \\(\\mathbf{y}_i = [y_{i1}, y_{i2}, \\dots, y_{iK}]\\) là label vector của nó, trong đó nếu \\( \\mathbf{x}_i \\) được phân vào cluster \\(k\\) thì  \\(y_{ik} = 1\\) và \\(y_{ij} = 0, \\forall j \\neq k \\). Điều này có nghĩa là có đúng một phần tử của vector \\(\\mathbf{y}_i\\) là bằng 1 (tương ứng với cluster của \\(\\mathbf{x}_i \\)), các phần tử còn lại bằng 0. Ví dụ: nếu một điểm dữ liệu có label vector là \\([1,0,0,\\dots,0]\\) thì nó thuộc vào cluster 1, là \\([0,1,0,\\dots,0]\\) thì nó thuộc vào cluster 2, \\(\\dots\\). Cách mã hóa label của dữ liệu như thế này được gọi là biểu diễn one-hot. Chúng ta sẽ thấy cách biểu diễn one-hot này rất phổ biến trong Machine Learning ở các bài tiếp theo.\nRàng buộc của \\(\\mathbf{y}_i \\) có thể viết dưới dạng toán học như sau:\n\\[\ny_{ik} \\in \\{0, 1\\},~~~ \\sum_{k = 1}^K y_{ik} = 1 ~~~ (1)\n\\]\nHàm mất mát và bài toán tối ưu\nNếu ta coi center \\(\\mathbf{m}_k \\)  là center (hoặc representative) của mỗi cluster và ước lượng tất cả các điểm được phân vào cluster này bởi \\(\\mathbf{m}_k \\), thì một điểm dữ liệu \\(\\mathbf{x}_i \\) được phân vào cluster \\(k\\) sẽ bị sai số là \\( (\\mathbf{x}_i - \\mathbf{m}_k) \\). Chúng ta mong muốn sai số này có trị tuyệt đối nhỏ nhất nên (giống như trong bài Linear Regression) ta sẽ tìm cách để đại lượng sau đây đạt giá trị nhỏ nhất:\n\\[\n\\|\\mathbf{x}_i - \\mathbf{m}_k\\|_2^2\n\\]\nHơn nữa, vì \\(\\mathbf{x}_i \\) được phân vào cluster \\(k\\) nên \\(y_{ik} = 1, y_{ij} = 0, ~\\forall j \\neq k \\). Khi đó, biểu thức bên trên sẽ được viết lại là:\n\\[\ny_{ik}\\|\\mathbf{x}_i - \\mathbf{m}_k\\|_2^2 =  \\sum_{j=1}^K y_{ij}\\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2\n\\]\n(Hy vọng chỗ này không quá khó hiểu)\nSai số cho toàn bộ dữ liệu sẽ là:\n\\[\n\\mathcal{L}(\\mathbf{Y}, \\mathbf{M}) = \\sum_{i=1}^N\\sum_{j=1}^K y_{ij} \\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2\n\\]\nTrong đó \\( \\mathbf{Y} = [\\mathbf{y}_1; \\mathbf{y}_2; \\dots; \\mathbf{y}_N]\\), \\( \\mathbf{M} = [\\mathbf{m}_1, \\mathbf{m}_2, \\dots \\mathbf{m}_K] \\) lần lượt là các ma trận được tạo bởi label vector của mỗi điểm dữ liệu và center của mỗi cluster. Hàm số mất mát trong bài toán K-means clustering của chúng ta là hàm \\(\\mathcal{L}(\\mathbf{Y}, \\mathbf{M})\\) với ràng buộc như được nêu trong phương trình \\((1)\\).\nTóm lại, chúng ta cần tối ưu bài toán sau:\n\\[\n\\mathbf{Y}, \\mathbf{M} = \\arg\\min_{\\mathbf{Y}, \\mathbf{M}} \\sum_{i=1}^N\\sum_{j=1}^K y_{ij} \\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2~~~~~(2)\n\\]\n\\[\n\\text{subject to:} ~~ y_{ij} \\in \\{0, 1\\}~~ \\forall i, j;~~~ \\sum_{j = 1}^K y_{ij} = 1~~\\forall i\n\\]\n(subject to nghĩa là thỏa mãn điều kiện).\nNhắc lại khái niệm \\(\\arg\\min\\): Chúng ta biết ký hiệu \\(\\min\\) là giá trị nhỏ nhất của hàm số, \\(\\arg\\min\\) chính là giá trị của biến số để hàm số đó đạt giá trị nhỏ nhất đó. Nếu \\(f(x) = x^2 -2x + 1 = (x-1)^2 \\) thì giá trị nhỏ nhất của hàm số này bằng 0, đạt được khi \\(x = 1\\). Trong ví dụ này \\(\\min_{x} f(x) = 0\\) và \\(\\arg\\min_{x} f(x) = 1\\). Thêm ví dụ khác, nếu \\(x_1 = 0, x_2 = 10, x_3 = 5\\) thì ta nói \\(\\arg\\min_{i} x_i = 1\\) vì \\(1\\) là chỉ số để \\(x_i\\) đạt giá trị nhỏ nhất (bằng \\(0\\)). Biến số viết bên dưới \\(\\min\\) là biến số cúng ta cần tối ưu. Trong các bài toán tối ưu, ta thường quan tâm tới \\(\\arg\\min\\) hơn là \\(\\min\\).\nThuật toán tối ưu hàm mất mát\nBài toán \\((2)\\) là một bài toán khó tìm điểm tối ưu vì nó có thêm các điều kiện ràng buộc. Bài toán này thuộc loại mix-integer programming (điều kiện biến là số nguyên) - là loại rất khó tìm nghiệm tối ưu toàn cục (global optimal point, tức nghiệm làm cho hàm mất mát đạt giá trị nhỏ nhất có thể). Tuy nhiên, trong một số trường hợp chúng ta vẫn có thể tìm được phương pháp để tìm được nghiệm gần đúng hoặc điểm cực tiểu. (Nếu chúng ta vẫn nhớ chương trình toán ôn thi đại học thì điểm cực tiểu chưa chắc đã phải là điểm làm cho hàm số đạt giá trị nhỏ nhất).\nMột cách đơn giản để giải bài toán \\((2)\\) là xen kẽ giải \\(\\mathbf{Y}\\) và \\( \\mathbf{M}\\) khi biến còn lại được cố định. Đây là một thuật toán lặp, cũng là kỹ thuật phổ biến khi giải bài toán tối ưu. Chúng ta sẽ lần lượt giải quyết hai bài toán sau đây:\nCố định \\(\\mathbf{M} \\), tìm \\(\\mathbf{Y}\\)\nGiả sử đã tìm được các centers, hãy tìm các label vector để hàm mất mát đạt giá trị nhỏ nhất. Điều này tương đương với việc tìm cluster cho mỗi điểm dữ liệu.\nKhi các centers là cố định, bài toán tìm label vector cho toàn bộ dữ liệu có thể được chia nhỏ thành bài toán tìm label vector cho từng điểm dữ liệu \\(\\mathbf{x}_i\\) như sau:\n\\[\n\\mathbf{y}_i = \\arg\\min_{\\mathbf{y}_i} \\sum_{j=1}^K y_{ij}\\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2 ~~~ (3)\n\\]\n\\[\n\\text{subject to:} ~~ y_{ij} \\in \\{0, 1\\}~~ \\forall j;~~~ \\sum_{j = 1}^K y_{ij} = 1\n\\]\nVì chỉ có một phần tử của label vector \\(\\mathbf{y}_i\\) bằng \\(1\\) nên bài toán \\((3)\\) có thể tiếp tục được viết dưới dạng đơn giản hơn:\n\\[\nj = \\arg\\min_{j} \\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2\n\\]\nVì \\(\\|\\mathbf{x}_i - \\mathbf{m}_j\\|_2^2\\) chính là bình phương khoảng cách tính từ điểm \\(\\mathbf{x}_i \\) tới center \\(\\mathbf{m}_j \\), ta có thể kết luận rằng mỗi điểm \\(\\mathbf{x}_i \\) thuộc vào cluster có center gần nó nhất! Từ đó ta có thể dễ dàng suy ra label vector của từng điểm dữ liệu.\nCố định \\(\\mathbf{Y} \\), tìm \\(\\mathbf{M}\\)\nGiả sử đã tìm được cluster cho từng điểm, hãy tìm center mới cho mỗi cluster để hàm mất mát đạt giá trị nhỏ nhất.\nMột khi chúng ta đã xác định được label vector cho từng điểm dữ liệu, bài toán tìm center cho mỗi cluster được rút gọn thành:\n\\[\n\\mathbf{m}_j = \\arg\\min_{\\mathbf{m}_j} \\sum_{i = 1}^{N} y_{ij}\\|\\mathbf{x}_i - \\mathbf{m}_j \\|_2^2.\n\\]\nTới đây, ta có thể tìm nghiệm bằng phương pháp giải đạo hàm bằng 0, vì hàm cần tối ưu là một hàm liên tục và có đạo hàm xác định tại mọi điểm. Và quan trọng hơn, hàm này là hàm convex (lồi) theo \\(\\mathbf{m}_j \\) nên chúng ta sẽ tìm được giá trị nhỏ nhất và điểm tối ưu tương ứng. Sau này nếu có dịp, tôi sẽ nói thêm về tối ưu lồi (convex optimization) - một mảng cực kỳ quan trọng trong toán tối ưu.\nĐặt \\(l(\\mathbf{m}_j)\\) là hàm bên trong dấu \\(\\arg\\min\\), ta có đạo hàm:\n\\[\n\\frac{\\partial l(\\mathbf{m}_j)}{\\partial \\mathbf{m}_j} = 2\\sum_{i=1}^N y_{ij}(\\mathbf{m}_j - \\mathbf{x}_i)\n\\]\nGiải phương trình đạo hàm bằng 0 ta có:\n\\[\n\\mathbf{m}_j \\sum_{i=1}^N y_{ij} = \\sum_{i=1}^N y_{ij} \\mathbf{x}_i\n\\]\n\\[\n\\Rightarrow \\mathbf{m}_j = \\frac{ \\sum_{i=1}^N y_{ij} \\mathbf{x}_i}{\\sum_{i=1}^N y_{ij}}\n\\]\nNếu để ý một chút, chúng ta sẽ thấy rằng mẫu số chính là phép đếm số lượng các điểm dữ liệu trong cluster \\(j\\) (Bạn có nhận ra không?). Còn tử số chính là tổng các điểm dữ liệu trong cluster \\(j\\). (Nếu bạn đọc vẫn nhớ điều kiện ràng buộc của các \\(y_{ij} \\) thì sẽ có thể nhanh chóng nhìn ra điều này).\nHay nói một cách đơn giản hơn nhiều: \\(\\mathbf{m}_j\\) là trung bình cộng của các điểm trong cluster \\(j\\).\nTên gọi K-means clustering cũng xuất phát từ đây.\nTóm tắt thuật toán\nTới đây tôi xin được tóm tắt lại thuật toán (đặc biệt quan trọng với các bạn bỏ qua phần toán học bên trên) như sau:\nĐầu vào: Dữ liệu \\(\\mathbf{X}\\) và số lượng cluster cần tìm \\(K\\).\nĐầu ra: Các center \\(\\mathbf{M}\\) và label vector cho từng điểm dữ liệu \\(\\mathbf{Y}\\).\nChọn \\(K\\) điểm bất kỳ làm các center ban đầu.\nPhân mỗi điểm dữ liệu vào cluster có center gần nó nhất.\nNếu việc gán dữ liệu vào từng cluster ở bước 2 không thay đổi so với vòng lặp trước nó thì ta dừng thuật toán.\nCập nhật center cho từng cluster bằng cách lấy trung bình cộng của tất các các điểm dữ liệu đã được gán vào cluster đó sau bước 2.\nQuay lại bước 2.\nChúng ta có thể đảm bảo rằng thuật toán sẽ dừng lại sau một số hữu hạn vòng lặp. Thật vậy, vì hàm mất mát là một số dương và sau mỗi bước 2 hoặc 3, giá trị của hàm mất mát bị giảm đi. Theo kiến thức về dãy số trong chương trình cấp 3: nếu một dãy số giảm và bị chặn dưới thì nó hội tụ! Hơn nữa, số lượng cách phân nhóm cho toàn bộ dữ liệu là hữu hạn nên đến một lúc nào đó, hàm mất mát sẽ không thể thay đổi, và chúng ta có thể dừng thuật toán tại đây.\nChúng ta sẽ có một vài thảo luận về thuật toán này, về những hạn chế và một số phương pháp khắc phục. Nhưng trước hết, hãy xem nó thể hiện như thế nào trong một ví dụ cụ thể dưới đây.\n3. Ví dụ trên Python\nGiới thiệu bài toán\nĐể kiểm tra mức độ hiểu quả của một thuật toán, chúng ta sẽ làm một ví dụ đơn giản (thường được gọi là toy example). Trước hết, chúng ta chọn center cho từng cluster và tạo dữ liệu cho từng cluster bằng cách lấy mẫu theo phân phối chuẩn có kỳ vọng là center của cluster đó và ma trận hiệp phương sai (covariance matrix) là ma trận đơn vị.\nTrước tiên, chúng ta cần khai báo các thư viện cần dùng. Chúng ta cần numpy và matplotlib như trong bài Linear Regression cho việc tính toán ma trận và hiển thị dữ liệu. Chúng ta cũng cần thêm thư viện scipy.spatial.distance để tính khoảng cách giữa các cặp điểm trong hai tập hợp một cách hiệu quả.\nfrom __future__ import print_function\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cdist\nnp.random.seed(11)\nTiếp theo, ta tạo dữ liệu bằng cách lấy các điểm theo phân phối chuẩn có kỳ vọng tại các điểm có tọa độ (2, 2), (8, 3) và (3, 6), ma trận hiệp phương sai giống nhau và là ma trận đơn vị. Mỗi cluster có 500 điểm. (Chú ý rằng mỗi điểm dữ liệu là một hàng của ma trận dữ liệu.\nmeans = [[2, 2], [8, 3], [3, 6]]\ncov = [[1, 0], [0, 1]]\nN = 500\nX0 = np.random.multivariate_normal(means[0], cov, N)\nX1 = np.random.multivariate_normal(means[1], cov, N)\nX2 = np.random.multivariate_normal(means[2], cov, N)\nX = np.concatenate((X0, X1, X2), axis = 0)\nK = 3\noriginal_label = np.asarray([0]*N + [1]*N + [2]*N).T\nHiển thị dữ liệu trên đồ thị\nChúng ta cần một hàm kmeans_display để hiển thị dữ liệu. Sau đó hiển thị dữ liệu theo nhãn ban đầu.\ndef kmeans_display(X, label):\nK = np.amax(label) + 1\nX0 = X[label == 0, :]\nX1 = X[label == 1, :]\nX2 = X[label == 2, :]\nplt.plot(X0[:, 0], X0[:, 1], 'b^', markersize = 4, alpha = .8)\nplt.plot(X1[:, 0], X1[:, 1], 'go', markersize = 4, alpha = .8)\nplt.plot(X2[:, 0], X2[:, 1], 'rs', markersize = 4, alpha = .8)\nplt.axis('equal')\nplt.plot()\nplt.show()\nkmeans_display(X, original_label)\nTrong đồ thị trên, mỗi cluster tương ứng với một màu. Có thể nhận thấy rằng có một vài điểm màu đỏ bị lẫn sang phần cluster màu xanh.\nCác hàm số cần thiết cho K-means clustering\nViết các hàm:\nkmeans_init_centers để khởi tạo các centers ban đầu.\nkmeans_asign_labels để gán nhán mới cho các điểm khi biết các centers.\nkmeans_update_centers để cập nhật các centers mới dữa trên dữ liệu vừa được gán nhãn.\nhas_converged để kiểm tra điều kiện dừng của thuật toán.\ndef kmeans_init_centers(X, k):\n# randomly pick k rows of X as initial centers\nreturn X[np.random.choice(X.shape[0], k, replace=False)]\ndef kmeans_assign_labels(X, centers):\n# calculate pairwise distances btw data and centers\nD = cdist(X, centers)\n# return index of the closest center\nreturn np.argmin(D, axis = 1)\ndef kmeans_update_centers(X, labels, K):\ncenters = np.zeros((K, X.shape[1]))\nfor k in range(K):\n# collect all points assigned to the k-th cluster\nXk = X[labels == k, :]\n# take average\ncenters[k,:] = np.mean(Xk, axis = 0)\nreturn centers\ndef has_converged(centers, new_centers):\n# return True if two sets of centers are the same\nreturn (set([tuple(a) for a in centers]) ==\nset([tuple(a) for a in new_centers]))\nPhần chính của K-means clustering:\ndef kmeans(X, K):\ncenters = [kmeans_init_centers(X, K)]\nlabels = []\nit = 0\nwhile True:\nlabels.append(kmeans_assign_labels(X, centers[-1]))\nnew_centers = kmeans_update_centers(X, labels[-1], K)\nif has_converged(centers[-1], new_centers):\nbreak\ncenters.append(new_centers)\nit += 1\nreturn (centers, labels, it)\nÁp dụng thuật toán vừa viết vào dữ liệu ban đầu, hiển thị kết quả cuối cùng.\n(centers, labels, it) = kmeans(X, K)\nprint('Centers found by our algorithm:')\nprint(centers[-1])\nkmeans_display(X, labels[-1])\nCenters found by our algorithm:\n[[ 1.97563391  2.01568065]\n[ 8.03643517  3.02468432]\n[ 2.99084705  6.04196062]]\nTừ kết quả này chúng ta thấy rằng thuật toán K-means clustering làm việc khá thành công, các centers tìm được khá gần với kỳ vọng ban đầu. Các điểm thuộc cùng một cluster hầu như được phân vào cùng một cluster (trừ một số điểm màu đỏ ban đầu đã bị phân nhầm vào cluster màu xanh da trời, nhưng tỉ lệ là nhỏ và có thể chấp nhận được).\nDưới đây là hình ảnh động minh họa thuật toán qua từng vòng lặp, chúng ta thấy rằng thuật toán trên hội tụ rất nhanh, chỉ cần 6 vòng lặp để có được kết quả cuối cùng:\nCác bạn có thể xem thêm các trang web minh họa thuật toán K-means cluster tại:\nVisualizing K-Means Clustering\nVisualizing K-Means Clustering - Standford\nKết quả tìm được bằng thư viện scikit-learn\nĐể kiểm tra thêm, chúng ta hãy so sánh kết quả trên với kết quả thu được bằng cách sử dụng thư viện scikit-learn.\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3, random_state=0).fit(X)\nprint('Centers found by scikit-learn:')\nprint(kmeans.cluster_centers_)\npred_label = kmeans.predict(X)\nkmeans_display(X, pred_label)\nCenters found by scikit-learn:\n[[ 8.0410628   3.02094748]\n[ 2.99357611  6.03605255]\n[ 1.97634981  2.01123694]]\nThật may mắn (cho tôi), hai thuật toán cho cùng một đáp số! Với cách thứ nhất, tôi mong muốn các bạn hiểu rõ được thuật toán K-means clustering làm việc như thế nào. Với cách thứ hai, tôi hy vọng các bạn biết áp dụng thư viện sẵn có như thế nào.\n4. Thảo luận\nHạn chế\nCó một vài hạn chế của thuật toán K-means clustering:\nChúng ta cần biết số lượng cluster cần clustering\nĐể ý thấy rằng trong thuật toán nêu trên, chúng ta cần biết đại lượng \\(K\\) là số lượng clusters. Trong thực tế, nhiều trường hợp chúng ta không xác định được giá trị này. Có một số phương pháp giúp xác định số lượng clusters, tôi sẽ dành thời gian nói về chúng sau nếu có dịp. Bạn đọc có thể tham khảo Elbow method - Determining the number of clusters in a data set.\nNghiệm cuối cùng phụ thuộc vào các centers được khởi tạo ban đầu\nTùy vào các center ban đầu mà thuật toán có thể có tốc độ hội tụ rất chậm, ví dụ:\nhoặc thậm chí cho chúng ta nghiệm không chính xác (chỉ là local minimum - điểm cực tiểu - mà không phải giá trị nhỏ nhất):\nCó một vài cách khắc phục đó là:\nChạy K-means clustering nhiều lần với các center ban đầu khác nhau rồi chọn cách có hàm mất mát cuối cùng đạt giá trị nhỏ nhất.\nK-means++ -Improve initialization algorithm - wiki.\nBạn nào muốn tìm hiểu sâu hơn có thể xem bài báo khoa học Cluster center initialization algorithm for K-means clustering.\nCác cluster cần có só lượng điểm gần bằng nhau\nDưới đây là một ví dụ với 3 cluster với 20, 50, và 1000 điểm. Kết quả cuối cùng không chính xác.\nCác cluster cần có dạng hình tròn\nTức các cluster tuân theo phân phối chuẩn và ma trận hiệp phương sai là ma trận đường chéo có các điểm trên đường chéo giống nhau.\nDưới đây là 1 ví dụ khi 1 cluster có dạng hình dẹt.\nKhi một cluster nằm phía trong 1 cluster khác\nĐây là ví dụ kinh điển về việc K-means clustering không thể phân cụm dữ liệu. Một cách tự nhiên, chúng ta sẽ phân ra thành 4 cụm: mắt trái, mắt phải, miệng, xung quanh mặt. Nhưng vì mắt và miệng nằm trong khuôn mặt nên K-means clustering không thực hiện được:\nMặc dù có những hạn chế, K-means clustering vẫn cực kỳ quan trọng trong Machine Learning và là nền tảng cho nhiều thuật toán phức tạp khác sau này. Chúng ta cần bắt đầu từ những thứ đơn giản. Simple is best!\n5. Tài liệu tham khảo\nClustering documents using k-means\nVoronoi Diagram - Wikipedia\nCluster center initialization algorithm for K-means clustering\nVisualizing K-Means Clustering\nVisualizing K-Means Clustering - Standford",
        "summary": "Thuật toán K-means clustering là một thuật toán phân cụm không giám sát, được sử dụng để phân chia dữ liệu thành các nhóm có tính chất giống nhau dựa trên khoảng cách đến các tâm cụm. Thuật toán này hoạt động bằng cách lặp đi lặp lại hai bước: gán nhãn cho các điểm dữ liệu dựa trên tâm cụm gần nhất và cập nhật tâm cụm dựa trên trung bình cộng của các điểm dữ liệu trong mỗi cụm.  K-means clustering là một thuật toán đơn giản và hiệu quả, nhưng nó có một số hạn chế, bao gồm việc cần biết trước số lượng cụm, kết quả phụ thuộc vào tâm cụm khởi tạo ban đầu, và không phù hợp với dữ liệu có hình dạng phức tạp hoặc có cụm lồng nhau. \n",
        "status": true
    },
    "ML006": {
        "content": "Những năm gần đây, AI - Artificial Intelligence (Trí Tuệ Nhân Tạo), và cụ thể hơn là Machine Learning (Học Máy hoặc Máy Học) nổi lên như một bằng chứng của cuộc cách mạng công nghiệp lần thứ tư (1 - động cơ hơi nước, 2 - năng lượng điện, 3 - công nghệ thông tin). Trí Tuệ Nhân Tạo đang len lỏi vào mọi lĩnh vực trong đời sống mà có thể chúng ta không nhận ra. Xe tự hành của Google và Tesla, hệ thống tự tag khuôn mặt trong ảnh của Facebook, trợ lý ảo Siri của Apple, hệ thống gợi ý sản phẩm của Amazon, hệ thống gợi ý phim của Netflix, máy chơi cờ vây AlphaGo của Google DeepMind, …, chỉ là một vài trong vô vàn những ứng dụng của AI/Machine Learning. (Xem thêm Jarvis - trợ lý thông minh cho căn nhà của Mark Zuckerberg)\n\nMachine Learning là một tập con của AI. Theo định nghĩa của Wikipedia, Machine learning is the subfield of computer science that “gives computers the ability to learn without being explicitly programmed”. Nói đơn giản, Machine Learning là một lĩnh vực nhỏ của Khoa Học Máy Tính, nó có khả năng tự học hỏi dựa trên dữ liệu đưa vào mà không cần phải được lập trình cụ thể. Bạn Nguyễn Xuân Khánh tại đại học Maryland đang viết một cuốn sách về Machine Learning bằng tiếng Việt khá thú vị, các bạn có thể tham khảo bài Machine Learning là gì?.\n\nNhững năm gần đây, khi mà khả năng tính toán của các máy tính được nâng lên một tầm cao mới và lượng dữ liệu khổng lồ được thu thập bởi các hãng công nghệ lớn, Machine Learning đã tiến thêm một bước dài và một lĩnh vực mới được ra đời gọi là Deep Learning (Học Sâu - thực sự tôi không muốn dịch từ này ra tiếng Việt). Deep Learning đã giúp máy tính thực thi những việc tưởng chừng như không thể vào 10 năm trước: phân loại cả ngàn vật thể khác nhau trong các bức ảnh, tự tạo chú thích cho ảnh, bắt chước giọng nói và chữ viết của con người, giao tiếp với con người, hay thậm chí cả sáng tác văn hay âm nhạc (Xem thêm 8 Inspirational Applications of Deep Learning)",
        "summary": "Trí tuệ nhân tạo (AI) và học máy (Machine Learning) đang thay đổi thế giới, từ xe tự hành đến trợ lý ảo, AI đã len lỏi vào mọi khía cạnh của cuộc sống. Machine Learning là một nhánh của AI cho phép máy tính tự học hỏi từ dữ liệu mà không cần được lập trình cụ thể, và Deep Learning, một lĩnh vực mới trong Machine Learning, đang giúp máy tính thực hiện những nhiệm vụ phức tạp như phân loại hình ảnh, tạo chú thích, bắt chước giọng nói và sáng tác văn học. \n",
        "status": true
    },
    "ML007": {
        "content": "1. Giới thiệu\nSắp đến kỳ thi, một cậu sinh viên tự đặt ra quy tắc học hay chơi của mình như sau. Nếu còn nhiều hơn hai ngày tới ngày thi, cậu ra sẽ đi chơi. Nếu còn không quá hai ngày và đêm hôm đó có một trận bóng đá, cậu sẽ sang nhà bạn chơi và cùng xem bóng đêm đó. Cậu sẽ chỉ học trong các trường hợp còn lại. Việc ra quyết định của cậu sinh viên này có thể được mô tả trên sơ đồ trong Hình 1. Hình ellipse nền vàng thể hiện quyết định cần được đưa ra. Quyết định này phụ thuộc vào các câu trả lời của các câu hỏi trong các ô hình chữ nhật màu xám. Dựa trên các câu trả lời, quyết định cuối cùng được cho trong các hình tròn màu lục (chơi) và đỏ (học). Sơ đồ trong Hình 1 còn được gọi là một cây quyết định.\n\n\tHình 1: Ví dụ về việc ra quyết định dựa trên các câu hỏi.\nViệc quan sát, suy nghĩ và ra các quyết định của con người thường được bắt đầu từ các câu hỏi. Machine learning cũng có một mô hình ra quyết định dựa trên các câu hỏi. Mô hình này có tên là cây quyết định (decision tree).\n\nXét ví dụ trên Hình 2a với hai class màu lục và đỏ trên không gian hai chiều. Nhiệm vụ là đi tìm ranh giới đơn giản giúp phân chia hai class này. Hay nói cách khác, đây là một bài toán classification, ta cần xây dựng một bộ phân lớp để quyết định việc một điểm dữ liệu mới thuộc vào class nào. Quan sát hình ta thấy rằng ranh giới cho hai class trong bài toán này khá đơn giản–chúng là các đường song song với các trục toạ độ. Nếu một điểm có thành phần thứ nhất, \nx\n1\n, nhỏ hơn ngưỡng \nt\n1\n, ta quyết định ngay được rằng nó thuộc class lục. Ngoài ra, nếu thành phần thứ hai, \nx\n2\n lớn hơn ngưỡng \nt\n2\n, ta quyết định nó cũng thuộc vào class lục. Xét tiếp, nếu thành phần thứ nhất, \nx\n1\n, lớn hơn ngưỡng \nt\n3\n, ta quyết định nó thuộc vào class lục. Các điểm không thoả mãn các điều kiện trên được xếp vào class đỏ. Việc ra quyết định một điểm thuộc class nào được mô tả trên decision tree trên Hình 2b.\n\n\n\n\nHình 2: Ví dụ về bài toán phân lớp sử dụng decision tree.\nTrong decision tree, các ô màu xám, lục, đỏ trên Hình 2 được gọi là các node. Các node thể hiện đầu ra (màu lục và đỏ) được gọi là node lá (leaf node hoặc terminal node). Các node thể hiện câu hỏi là các non-leaf node. Non-leaf node trên cùng (câu hỏi đầu tiên) được gọi là node gốc (root node). Các non-leaf node thường có hai hoặc nhiều node con (child node). Các child node này có thể là một leaf node hoặc một non-leaf node khác. Các child node có cùng bố mẹ được gọi là sibling node. Nếu tất cả các non-leaf node chỉ có hai child node, ta nói rằng đó là một binary decision tree (cây quyết định nhị phân). Các câu hỏi trong binary decision tree đều có thể đưa được về dạng câu hỏi đúng hay sai. Các decision tree mà một leaf node có nhiều child node cũng có thể được đưa về dạng một binary decision tree. Điều này có thể đạt được vì hầu hết các câu hỏi đều có thể được đưa về dạng câu hỏi đúng sai.\n\nVí dụ, ta có thể xác định được tuổi của một người dựa trên nhiều câu hỏi đúng sai dạng: tuổi của bạn lớn hơn \nx\n đúng không? (Đây chính là thuật toán tìm kiếm nhị phân – binary search.)\n\nDecision tree là một mô hình supervised learning, có thể được áp dụng vào cả hai bài toán classification và regression. Việc xây dựng một decision tree trên dữ liệu huấn luyện cho trước là việc đi xác định các câu hỏi và thứ tự của chúng. Một điểm đáng lưu ý của decision tree là nó có thể làm việc với các đặc trưng (trong các tài liệu về decision tree, các đặc trưng thường được gọi là thuộc tính – attribute) dạng categorical, thường là rời rạc và không có thứ tự. Ví dụ, mưa, nắng hay xanh, đỏ, v.v. Decision tree cũng làm việc với dữ liệu có vector đặc trưng bao gồm cả thuộc tính dạng categorical và liên tục (numeric). Một điểm đáng lưu ý nữa là decision tree ít yêu cầu việc chuẩn hoá dữ liệu.\n\nTrong bài viết này, chúng ta sẽ làm quen với một thuật toán xây dựng decision tree ra đời từ rất sớm và rất phổ biến: Iterative Dichotomiser 3 (ID3).\n\nQuay trở lại với nhiệm vụ chính của việc xây dựng một decision tree: các câu hỏi nên được xây dựng như thế nào, và thứ tự của chúng ra sao. Các câu hỏi này thường được áp dụng lên từng thuộc tính, hoặc một tổ hợp tuyến tính của các thuộc tính. Cách thứ nhất, áp dụng lên từng thuộc tính, được sử dụng nhiều hơn vì tính đơn giản của nó. Với các thuộc tính dạng categorical, câu hỏi sẽ là Nó rơi vào category nào? hoặc Nó có rơi vào category nào đó không? với trường hợp nhị phân. Với các thuộc tính dạng liên tục, câu hỏi có thể là Nó nằm vào khoảng giá trị nào? hoặc Nó có lớn hơn một ngưỡng nào đó không?.\n\nID3 là một thuật toán decision tree được áp dụng cho các bài toán classification mà tất cả các thuộc tính đều ở dạng categorical. Trong bài tiếp theo, chúng ta sẽ làm quen với một thuật toán khác có tên là Classification and Regression Tree (CART)–có thể được áp dụng vào cả hai loại classification và regression, như tên gọi của nó–làm việc với cả thuộc tính dạng categorical và liên tục.\n\n\n2. ID3\n\n2.1. Ý tưởng\nTrong ID3, chúng ta cần xác định thứ tự của thuộc tính cần được xem xét tại mỗi bước. Với các bài toán có nhiều thuộc tính và mỗi thuộc tính có nhiều giá trị khác nhau, việc tìm được nghiệm tối ưu thường là không khả thi. Thay vào đó, một phương pháp đơn giản thường được sử dụng là tại mỗi bước, một thuộc tính tốt nhất sẽ được chọn ra dựa trên một tiêu chuẩn nào đó (chúng ta sẽ bàn sớm). Với mỗi thuộc tính được chọn, ta chia dữ liệu vào các child node tương ứng với các giá trị của thuộc tính đó rồi tiếp tục áp dụng phương pháp này cho mỗi child node. Việc chọn ra thuộc tính tốt nhất ở mỗi bước như thế này được gọi là cách chọn greedy (tham lam). Cách chọn này có thể không phải là tối ưu, nhưng trực giác cho chúng ta thấy rằng cách làm này sẽ gần với cách làm tối ưu. Ngoài ra, cách làm này khiến cho bài toán cần giải quyết trở nên đơn giản hơn.\n\nSau mỗi câu hỏi, dữ liệu được phân chia vào từng child node tương ứng với các câu trả lời cho câu hỏi đó. Câu hỏi ở đây chính là một thuộc tính, câu trả lời chính là giá trị của thuộc tính đó. Để đánh giá chất lượng của một cách phân chia, chúng ta cần đi tìm một phép đo.\n\nTrước hết, thế nào là một phép phân chia tốt? Bằng trực giác, một phép phân chia là tốt nhất nếu dữ liệu trong mỗi child node hoàn toàn thuộc vào một class–khi đó child node này có thể được coi là một leaf node, tức ta không cần phân chia thêm nữa. Nếu dữ liệu trong các child node vẫn lẫn vào nhau theo tỉ lệ lớn, ta coi rằng phép phân chia đó chưa thực sự tốt. Từ nhận xét này, ta cần có một hàm số đo độ tinh khiết (purity), hoặc độ vẩn đục (impurity) của một phép phân chia. Hàm số này sẽ cho giá trị thấp nhất nếu dữ liệu trong mỗi child node nằm trong cùng một class (tinh khiết nhất), và cho giá trị cao nếu mỗi child node có chứa dữ liệu thuộc nhiều class khác nhau.\n\nMột hàm số có các đặc điểm này và được dùng nhiều trong lý thuyết thông tin là hàm entropy.\n\n\n2.2. Hàm số entropy\nCho một phân phối xác suất của một biến rời rạc \nx\n có thể nhận \nn\n giá trị khác nhau \nx\n1\n,\nx\n2\n,\n…\n,\nx\nn\n. Giả sử rằng xác suất để \nx\n nhận các giá trị này là \np\ni\n=\np\n(\nx\n=\nx\ni\n)\n với \n0\n≤\np\ni\n≤\n1\n,\n∑\nn\ni\n=\n1\np\ni\n=\n1\n. Ký hiệu phân phối này là \np\n=\n(\np\n1\n,\np\n2\n,\n…\n,\np\nn\n)\n. Entropy của phân phối này được định nghĩa là\nH\n(\np\n)\n=\n−\nn\n∑\ni\n=\n1\n \np\ni\nlog\n(\np\ni\n)\n(\n1\n)\ntrong đó \nlog\n là logarit tự nhiên (Một số tài liệu dùng logarit cơ số 2, nhưng giá trị của \nH\n(\np\n)\n chỉ khác đi bằng cách nhân với một hằng số.) và quy ước \n0\nlog\n(\n0\n)\n=\n0\n.\n\nXét một ví dụ với \nn\n=\n2\n được cho trên Hình 3. Trong trường hợp \np\n là tinh khiết nhất, tức một trong hai giá trị \np\ni\n bằng 1, giá trị kia bằng 0, entropy của phân phối này là \nH\n(\np\n)\n=\n0\n. Khi \np\n là vẩn đục nhất, tức cả hai giá trị \np\ni\n=\n0.5\n, hàm entropy đạt giá trị cao nhất.\n\n\tHình 3: Đồ thị của hàm entropy với \nn\n=\n2\n.\nTổng quát lên với \nn\n>\n2\n, hàm entropy đạt giá trị nhỏ nhất nếu có một giá trị \np\ni\n=\n1\n, đạt giá trị lớn nhất nếu tất cả các \np\ni\n bằng nhau ((việc này có thể được chứng minh bằng phương pháp nhân tử Lagrange).\n\nNhững tính chất này của hàm entropy khiến nó được sử dụng trong việc đo độ vẩn đục của một phép phân chia của ID3. Vì lý do này, ID3 còn được gọi là entropy-based decision tree.\n\n\n2.3. Thuật toán ID3\nTrong ID3, tổng có trọng số của entropy tại các leaf-node sau khi xây dựng decision tree được coi là hàm mất mát của decision tree đó. Các trọng số ở đây tỉ lệ với số điểm dữ liệu được phân vào mỗi node. Công việc của ID3 là tìm các cách phân chia hợp lý (thứ tự chọn thuộc tính hợp lý) sao cho hàm mất mát cuối cùng đạt giá trị càng nhỏ càng tốt. Như đã đề cập, việc này đạt được bằng cách chọn ra thuộc tính sao cho nếu dùng thuộc tính đó để phân chia, entropy tại mỗi bước giảm đi một lượng lớn nhất. Bài toán xây dựng một decision tree bằng ID3 có thể chia thành các bài toán nhỏ, trong mỗi bài toán, ta chỉ cần chọn ra thuộc tính giúp cho việc phân chia đạt kết quả tốt nhất. Mỗi bài toán nhỏ này tương ứng với việc phân chia dữ liệu trong một non-leaf node. Chúng ta sẽ xây dựng phương pháp tính toán dựa trên mỗi node này.\n\nXét một bài toán với \nC\n class khác nhau. Giả sử ta đang làm việc với một non-leaf node với các điểm dữ liệu tạo thành một tập \nS\n với số phần tử là \n|\nS\n|\n=\nN\n. Giả sử thêm rằng trong số \nN\n điểm dữ liệu này, \nN\nc\n,\nc\n=\n1\n,\n2\n,\n…\n,\nC\n điểm thuộc vào class \nc\n. Xác suất để mỗi điểm dữ liệu rơi vào một class \nc\n được xấp xỉ bằng \nN\nc\nN\n (maximum likelihood estimation). Như vậy, entropy tại node này được tính bởi:\nH\n(\nS\n)\n=\n−\nC\n∑\nc\n=\n1\n \nN\nc\nN\nlog\n(\nN\nc\nN\n)\n(\n2\n)\nTiếp theo, giả sử thuộc tính được chọn là \nx\n. Dựa trên \nx\n, các điểm dữ liệu trong \nS\n được phân ra thành \nK\n child node \nS\n1\n,\nS\n2\n,\n…\n,\nS\nK\n với số điểm trong mỗi child node lần lượt là \nm\n1\n,\nm\n2\n,\n…\n,\nm\nK\n. Ta định nghĩa\n\nH\n(\nx\n,\nS\n)\n=\nK\n∑\nk\n=\n1\n \nm\nk\nN\nH\n(\nS\nk\n)\n(\n3\n)\nlà tổng có trọng số entroy của mỗi child node–được tính tương tự như (2). Việc lấy trọng số này là quan trọng vì các node thường có số lượng điểm khác nhau.\n\nTiếp theo, ta định nghĩa information gain dựa trên thuộc tính \nx\n:\n\nG\n(\nx\n,\nS\n)\n=\nH\n(\nS\n)\n−\nH\n(\nx\n,\nS\n)\nTrong ID3, tại mỗi node, thuộc tính được chọn được xác định dựa trên:\n\nx\n∗\n=\narg\nmax\nx\n \nG\n(\nx\n,\nS\n)\n=\narg\nmin\nx\n \nH\n(\nx\n,\nS\n)\ntức thuộc tính khiến cho information gain đạt giá trị lớn nhất.\n\nCâu hỏi tiếp theo là khi nào thì dừng cách phân chia? Câu trả lời sẽ được đề cập sau mục ví dụ dưới đây.\n\n\n2.4. Ví dụ\nĐể mọi thứ được rõ ràng hơn, chúng ta cùng xem ví dụ với dữ liệu huấn luyện được cho trong Bảng dưới đây. Bảng dữ liệu này được lấy từ cuốn sách Data Mining: Practical Machine Learning Tools and Techniques, trang 11. Đây là một bảng dữ liệu được sử dụng rất nhiều trong các bài giảng về decision tree. Bảng dữ liệu này mô tả mối quan hệ giữa thời tiết trong 14 ngày (bốn cột đầu, không tính cột id) và việc một đội bóng có chơi bóng hay không (cột cuối cùng). Nói cách khác, ta phải dự đoán giá trị ở cột cuối cùng nếu biết giá trị của bốn cột còn lại.\n\nid\toutlook\ttemperature\thumidity\twind\tplay\n1\tsunny\thot\thigh\tweak\tno\n2\tsunny\thot\thigh\tstrong\tno\n3\tovercast\thot\thigh\tweak\tyes\n4\trainy\tmild\thigh\tweak\tyes\n5\trainy\tcool\tnormal\tweak\tyes\n6\trainy\tcool\tnormal\tstrong\tno\n7\tovercast\tcool\tnormal\tstrong\tyes\n8\tsunny\tmild\thigh\tweak\tno\n9\tsunny\tcool\tnormal\tweak\tyes\n10\trainy\tmild\tnormal\tweak\tyes\n11\tsunny\tmild\tnormal\tstrong\tyes\n12\tovercast\tmild\thigh\tstrong\tyes\n13\tovercast\thot\tnormal\tweak\tyes\n14\trainy\tmild\thigh\tstrong\tno\nCó bốn thuộc tính thời tiết:\n\nOutlook nhận một trong ba giá trị: sunny, overcast, rainy.\n\nTemperature nhận một trong ba giá trị: hot, cool, mild.\n\nHumidity nhận một trong hai giá trị: high, normal.\n\nWind nhận một trong hai giá trị: weak, strong.\n\n(Tổng cộng có \n3\n×\n3\n×\n2\n×\n2\n=\n36\n loại thời tiết khác nhau, trong đó 14 loại được thể hiện trong bảng.)\n\nĐây có thể được coi là một bài toán dự đoán liệu đội bóng có chơi bóng không dựa trên các quan sát thời tiết. Ở đây, các quan sát đều ở dạng categorical. Cách dự đoán dưới đây tương đối đơn giản và khá chính xác, có thể không phải là cách ra quyết định tốt nhất:\n\nNếu outlook = sunny và humidity = high thì play = no.\n\nNếu outlook = rainy và windy = true thì play = no.\n\nNếu outlook = overcast thì play = yes.\n\nNgoài ra, nếu humidity = normal thì play = yes.\n\nNgoài ra, play = yes.\n\nChúng ta sẽ cùng tìm thứ tự các thuộc tính bằng thuật toán ID3.\n\nTrong 14 giá trị đầu ra ở Bảng trên, có năm giá trị bằng no và chín giá trị bằng yes. Entroy tại root node của bài toán là:\nH\n(\nS\n)\n=\n−\n5\n14\nlog\n(\n5\n14\n)\n−\n9\n14\nlog\n(\n9\n14\n)\n≈\n0.65\nTiếp theo, chúng ta tính tổng có trọng số entropy của các child node nếu chọn một trong các thuộc tính outlook, temperature, humidity, wind, play để phân chia dữ liệu.\n\nXét thuộc tính outlook. Thuộc tính này có thể nhận một trong ba giá trị sunny, overcast, rainy. Mỗi một giá trị sẽ tương ứng với một child node. Gọi tập hợp các điểm trong mỗi child node này lần lượt là \nS\ns\n,\nS\no\n,\nS\nr\n với tương ứng \nm\ns\n,\nm\no\n,\nm\nr\n phần tử. Sắp xếp lại Bảng ban đầu theo thuộc tính outlook ta đạt được ba Bảng nhỏ sau đây.\n\nid\toutlook\ttemperature\thumidity\twind\tplay\n1\tsunny\thot\thigh\tweak\tno\n2\tsunny\thot\thigh\tstrong\tno\n8\tsunny\tmild\thigh\tweak\tno\n9\tsunny\tcool\tnormal\tweak\tyes\n11\tsunny\tmild\tnormal\tstrong\tyes\nid\toutlook\ttemperature\thumidity\twind\tplay\n3\tovercast\thot\thigh\tweak\tyes\n7\tovercast\tcool\tnormal\tstrong\tyes\n12\tovercast\tmild\thigh\tstrong\tyes\n13\tovercast\thot\tnormal\tweak\tyes\nid\toutlook\ttemperature\thumidity\twind\tplay\n4\trainy\tmild\thigh\tweak\tyes\n5\trainy\tcool\tnormal\tweak\tyes\n6\trainy\tcool\tnormal\tstrong\tno\n10\trainy\tmild\tnormal\tweak\tyes\n14\trainy\tmild\thigh\tstrong\tno\nQuan sát nhanh ta thấy rằng child node ứng với outlook = overcast sẽ có entropy bằng 0 vì tất cả \nm\no\n=\n4\n output đều là yes. Hai child node còn lại với \nm\ns\n=\nm\nr\n=\n5\n có entropy khá cao vì tần suất output bằng yes hoặc no là xấp xỉ nhau. Tuy nhiên, hai child node này có thể được phân chia tiếp dựa trên hai thuộc tính humidity và wind.\n\nBạn đọc có thể kiểm tra được rằng\nH\n(\nS\ns\n)\n=\n−\n2\n5\nlog\n(\n2\n5\n)\n−\n3\n5\nlog\n(\n3\n5\n)\n≈\n0.673\nH\n(\nS\no\n)\n=\n0\nH\n(\nS\nr\n)\n=\n−\n3\n5\nlog\n(\n2\n5\n)\n−\n3\n5\nlog\n(\n3\n5\n)\n≈\n0.673\nH\n(\no\nu\nt\nl\no\no\nk\n,\nS\n)\n=\n5\n14\nH\n(\nS\ns\n)\n+\n4\n14\nH\n(\nS\no\n)\n+\n5\n14\nH\n(\nS\nr\n)\n≈\n0.48\nXét thuộc tính temperature, ta có phân chia như các Bảng dưới đây.\n\nid\toutlook\ttemperature\thumidity\twind\tplay\n1\tsunny\thot\thigh\tweak\tno\n2\tsunny\thot\thigh\tstrong\tno\n3\tovercast\thot\thigh\tweak\tyes\n13\tovercast\thot\tnormal\tweak\tyes\nid\toutlook\ttemperature\thumidity\twind\tplay\n4\trainy\tmild\thigh\tweak\tyes\n8\tsunny\tmild\thigh\tweak\tno\n10\trainy\tmild\tnormal\tweak\tyes\n11\tsunny\tmild\tnormal\tstrong\tyes\n12\tovercast\tmild\thigh\tstrong\tyes\n14\trainy\tmild\thigh\tstrong\tno\nid\toutlook\ttemperature\thumidity\twind\tplay\n5\trainy\tcool\tnormal\tweak\tyes\n6\trainy\tcool\tnormal\tstrong\tno\n7\tovercast\tcool\tnormal\tstrong\tyes\n9\tsunny\tcool\tnormal\tweak\tyes\nGọi \nS\nh\n,\nS\nm\n,\nS\nc\n là ba tập con tương ứng với temperature bằng hot, mild, cool. Bạn đọc có thể tính được\nH\n(\nS\nh\n)\n=\n−\n2\n4\nlog\n(\n2\n4\n)\n−\n2\n4\nlog\n(\n2\n4\n)\n≈\n0.693\nH\n(\nS\nm\n)\n=\n−\n4\n6\nlog\n(\n4\n6\n)\n−\n2\n6\nlog\n(\n2\n6\n)\n≈\n0.637\nH\n(\nS\nc\n)\n=\n−\n3\n4\nlog\n(\n3\n4\n)\n−\n1\n4\nlog\n(\n1\n4\n)\n≈\n0.562\nH\n(\nt\ne\nm\np\ne\nr\na\nt\nu\nr\ne\n,\nS\n)\n=\n4\n14\nH\n(\nS\nh\n)\n+\n6\n14\nH\n(\nS\nm\n)\n+\n4\n14\nH\n(\nS\nc\n)\n≈\n0.631\nViệc tính toán với hai thuộc tính còn lại được dành cho bạn đọc. Nếu các kết quả là giống nhau, chúng sẽ bằng:\nH\n(\nh\nu\nm\ni\nd\ni\nt\ny\n,\nS\n)\n≈\n0.547\n,\nH\n(\nw\ni\nn\nd\n,\nS\n)\n≈\n0.618\nNhư vậy, thuộc tính cần chọn ở bước đầu tiên là outlook vì \nH\n(\no\nu\nt\nl\no\no\nk\n,\nS\n)\n đạt giá trị nhỏ nhất (information gain là lớn nhất).\n\nSau bước phân chia đầu tiên này, ta nhận được ba child node với các phần tử như trong ba Bảng phân chia theo outlook. Child node thứ hai không cần phân chia tiếp vì nó đã tinh khiết. Với child node thứ nhất, ứng với outlook = sunny, kết quả tính được bằng ID3 sẽ cho chúng ta thuộc tính humidity vì tổng trọng số của entropy sau bước này sẽ bằng 0 với output bằng yes khi và chỉ khi humidity = normal. Tương tự, child node ứng với outlook = wind sẽ được tiếp tục phân chia bởi thuộc tính wind với output bằng yes khi và chỉ khi wind = weak.\n\nNhư vậy, cây quyết định cho bài toán này dựa trên ID3 sẽ có dạng như trong Hình 4.\n\n\tHình 4: Decision tree cho bài toán ví dụ sử dụng thuật toán ID3.\n\n2.5. Điều kiện dừng\nTrong các thuật toán decision tree nói chung và ID3 nói riêng, nếu ta tiếp tục phân chia các node chưa tinh khiết, ta sẽ thu được một tree mà mọi điểm trong tập huấn luyện đều được dự đoán đúng (giả sử rằng không có hai input giống nhau nào cho output khác nhau). Khi đó, tree có thể sẽ rất phức tạp (nhiều node) với nhiều leaf node chỉ có một vài điểm dữ liệu. Như vậy, nhiều khả năng overfitting sẽ xảy ra.\n\nĐể tránh overfitting, một trong số các phương pháp sau có thể được sử dụng. Tại một node, nếu một trong số các điều kiện sau đây xảy ra, ta không tiếp tục phân chia node đó và coi nó là một leaf node:\n\nnếu node đó có entropy bằng 0, tức mọi điểm trong node đều thuộc một class.\n\nnếu node đó có số phần tử nhỏ hơn một ngưỡng nào đó. Trong trường hợp này, ta chấp nhận có một số điểm bị phân lớp sai để tránh overfitting. Class cho leaf node này có thể được xác định dựa trên class chiếm đa số trong node.\n\nnếu khoảng cách từ node đó đến root node đạt tới một giá trị nào đó. Việc hạn chế chiều sâu của tree này làm giảm độ phức tạp của tree và phần nào giúp tránh overfitting.\n\nnếu tổng số leaf node vượt quá một ngưỡng nào đó.\n\nnếu việc phân chia node đó không làm giảm entropy quá nhiều (information gain nhỏ hơn một ngưỡng nào đó).\n\nNgoài các phương pháp trên, một phương pháp phổ biến khác được sử dụng để tránh overfitting là pruning, tạm dịch là cắt tỉa.\n\n\n2.6. Pruning\nPruning là một kỹ thuật regularization để tránh overfitting cho decision tree nói chung. Trong pruning, một decision tree sẽ được xây dựng tới khi mọi điểm trong training set đều được phân lớp đúng. Sau đó, các leaf node có chung một non-leaf node sẽ được cắt tỉa và non-leaf node đó trở thành một leaf-node, với class tương ứng với class chiếm đa số trong số mọi điểm được phân vào node đó. Việc cắt tỉa cây quyết định này có thể được xác định dựa vào các cách sau.\n\nDựa vào một validation set. Trước tiên, training set được tách ra thành một training set nhỏ hơn và một validation set. Decision tree được xây dựng trên training set cho tới khi mọi điểm trong training set được phân lớp đúng. Sau đó, đi ngược từ các leaf node, cắt tỉa các sibling node của nó và giữ lại node bố mẹ nếu độ chính xác trên validation set được cải thiện. Khi nào độ chính xác trên validation set không được cải thiện nữa, quá trình pruning dừng lại. Phương pháp này còn được gọi là reduced error pruning.\n\nDựa vào toàn bộ data set. Trong phương pháp này, ta không tách tập training ban đầu ra mà sử dụng toàn bộ dữ liệu trong tập này cho việc xây dựng decision tree. Một ví dụ cho việc này là cộng thêm một đại lượng regularization vào hàm mất mát. Đại lượng regularization sẽ lớn nếu số leaf node là lớn. Cụ thể, giả sử decision tree cuối cùng có \nK\n leaf node, tập hợp các điểm huấn luyện rơi vào mỗi leaf node lần lượt là \nS\n1\n,\n…\n,\nS\nK\n. Khi đó, regularized loss của ID3 có thể được tính tương tự như (3):\nL\n=\nK\n∑\nk\n=\n1\n \n|\nS\nk\n|\n|\nS\n|\nH\n(\nS\nk\n)\n+\nλ\nK\n(\n5\n)\nvới \n|\nS\nk\n|\n ký hiệu số phần tử của tập hợp \nS\nk\n và \nH\n(\nS\nk\n)\n chính là entropy của leaf node tương ứng với \nS\nk\n, được tính tương tự như (2), và \nλ\n là một số thực dương không quá lớn. Giá trị của hàm số này nhỏ nếu cả data loss–số hạng thứ nhất–nhỏ (entropy tại mỗi node là thấp) và regularization–số hạng thứ hai–cũng nhỏ (số leaf node là ít). Vì hàm mất mát trong (5) là một hàm rời rạc, rất khó để trực tiếp tối ưu hàm này. Việc tối ưu có thể được thực hiện thông qua pruning như sau. Trước hết, xây dựng một decision tree mà mọi điểm trong tập huấn luyện đều được phân loại đúng (toàn bộ các entopy của các node bằng 0). Lúc này data loss bằng 0 nhưng regularization có thể lớn, khiến cho \nL\n lớn. Sau đó, ta có thể tỉa dần các leaf node sao cho \nL\n giảm. Việc cắt tỉa được lặp lại đến khi \nL\n không thể giảm được nữa.\n\nCác kỹ thuật pruning khác có thể được tìm thấy tại đây.\n\n\n3. Lập trình Python cho ID3\nModule DecisionTree trong sklearn không thực hiện thuật toán ID3 mà là một thuật toán khác được đề cập trong bài tiếp theo. Phiên bản hiện tại trong sklearn chưa hỗ trợ các thuộc tính ở dạng categorical. Với dữ liệu có thuộc tính categorical, cách thường dùng là chuyển đổi các thuộc tính đó sang dạng numerical (1, 2, 3 cho mỗi giá trị). Chẳng hạn, các giá trị hot, mild, cool có thể lần lượt được thay bằng 1, 2, 3. Cách làm này có hạn chế vì trong cách chuyển đổi này, mild là trung bình cộng của hot và cool, nhưng nếu thứ tự các giá trị được đặt khác đi, việc chuyển đổi có thể ảnh hưởng lớn tới kết quả. Nhắc lại rằng các thuộc tính categorical, ví dụ màu sắc, thường không có tính thứ tự.\n\nDưới đây là cách lập trình của tôi cho ID3, làm việc với cả dữ liệu ở dạng categorical. (Source code có thể được tìm thấy tại đây)\n\nXây dựng class TreeNode\n\nfrom __future__ import print_function \nimport numpy as np \nimport pandas as pd \n\nclass TreeNode(object):\n    def __init__(self, ids = None, children = [], entropy = 0, depth = 0):\n        self.ids = ids           # index of data in this node\n        self.entropy = entropy   # entropy, will fill later\n        self.depth = depth       # distance to root node\n        self.split_attribute = None # which attribute is chosen, it non-leaf\n        self.children = children # list of its child nodes\n        self.order = None       # order of values of split_attribute in children\n        self.label = None       # label of node if it is a leaf\n\n    def set_properties(self, split_attribute, order):\n        self.split_attribute = split_attribute # split at which attribute\n        self.order = order # order of this node's children \n\n    def set_label(self, label):\n        self.label = label # set label if the node is a leaf \nHàm tính entropy dựa trên tần suất\n\nTrong hàm này, chúng ta phải chú ý bỏ các tần suất bằng 0 đi vì logarit tại đây không xác định.\n\ndef entropy(freq):\n    # remove prob 0 \n    freq_0 = freq[np.array(freq).nonzero()[0]]\n    prob_0 = freq_0/float(freq_0.sum())\n    return -np.sum(prob_0*np.log(prob_0))\nPhần còn lại của source code (bao gồm class DecisionTreeID3) có thể được tìm thấy tại đây.\n\nDữ liệu trong trong ví dụ được được lưu trong file weather.csv. Việc huấn luyện decision tree dựa trên ID3 cho tập dữ liệu này và đầu ra dự đoán cho training set được cho bởi\n\ndf = pd.DataFrame.from_csv('weather.csv')\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\ntree = DecisionTreeID3(max_depth = 3, min_samples_split = 2)\ntree.fit(X, y)\nprint(tree.predict(X))\nKết quả\n\n['no', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no']\nKhông có gì bất ngờ, decision tree dự đoán đúng 100% các điểm trong training set.\n\n\n4. Thảo luận\nNếu một thuộc tính có thể nhận rất nhiều giá trị, decision tree thu được có thể sẽ có rất nhiều node. Xét một ví dụ về các triệu chứng của các bệnh nhân trong một bệnh viện và đầu ra là mắc bệnh hay không. Mỗi bệnh nhân có một mã số (id) khác nhau. Nếu ta sử dụng thuộc tính này cho việc huấn luyện, ta rất có thể sẽ thu được mộ decision tree mà mỗi leaf node ứng với một bệnh nhân. Lúc đó mô hình này là vô dụng, vì không thể dự đoán được việc mắc bệnh hay không của một bệnh nhân mới.\n\nKhi một thuộc tính nhận giá trị liên tục, chẳng hạn temperature không còn là hot, mild, cool nữa mà là các giá trị thực liên tục, vẫn có một cách để áp dụng ID3. Ta có thể chia khoảng giá trị của thuộc tính này thành nhiều phần, mỗi phần có số lượng điểm tương đương, hoặc cũng có thể dùng các thuật toán clustering đơn giản cho một chiều dữ liệu để chia thuộc tính thành các cluster nhỏ. Lúc này, thuộc tính liên tục được chuyển về thuộc tính dạng categorical.\n\nHạn chế lớn nhất của ID3 và decision tree nói chung là việc nếu một điểm dữ liệu mới rơi vào nhầm nhánh ở ngay những lần phân chia đầu tiên, kết quả cuối cùng sẽ khác đi rất nhiều. Việc rơi vào nhầm nhánh này rất dễ xảy ra trong trường hợp thuộc tính liên tục được chia thành nhiều nhóm nhỏ, vì hai điểm có thuộc tính tương ứng rất gần nhau có thể rơi vào hai nhóm khác nhau.\n\n\n5. Tài liệu tham khảo\n[1] CSE5230 Tutorial: The ID3 Decision Tree Algorithm.\n\n[2] Hands-On Machine Learning with Scikit-Learn and TensorFlow\n\n",
        "summary": "Bài viết giới thiệu về cây quyết định (decision tree), một mô hình học máy được sử dụng để giải quyết các bài toán phân loại (classification) và hồi quy (regression). Cây quyết định hoạt động bằng cách đưa ra các câu hỏi liên tiếp dựa trên các thuộc tính của dữ liệu, mỗi câu hỏi chia dữ liệu vào các nhánh con tương ứng với các câu trả lời. Thuật toán ID3 là một thuật toán xây dựng cây quyết định dựa trên entropy, chọn thuộc tính để phân chia dữ liệu sao cho entropy của các nhánh con giảm đi một lượng lớn nhất.\nBài viết trình bày chi tiết thuật toán ID3, bao gồm ý tưởng, hàm entropy, cách thức hoạt động của thuật toán, điều kiện dừng và kỹ thuật pruning để tránh overfitting. Bài viết cũng cung cấp ví dụ minh họa cách thức hoạt động của ID3 và code Python để xây dựng cây quyết định dựa trên thuật toán này.\nBài viết cũng thảo luận về một số hạn chế của ID3 và cây quyết định nói chung, bao gồm việc dễ bị ảnh hưởng bởi việc phân chia dữ liệu vào các nhánh con sai ở những lần phân chia đầu tiên, đặc biệt là khi thuộc tính liên tục được chia thành nhiều nhóm nhỏ. \n",
        "status": true
    },
    "ML008": {
        "content": "Overfitting không phải là một thuật toán trong Machine Learning. Nó là một hiện tượng không mong muốn thường gặp, người xây dựng mô hình Machine Learning cần nắm được các kỹ thuật để tránh hiện tượng này.\n\n\n\n1. Giới thiệu\nĐây là một câu chuyện của chính tôi khi lần đầu biết đến Machine Learning.\n\nNăm thứ ba đại học, một thầy giáo có giới thiệu với lớp tôi về Neural Networks. Lần đầu tiên nghe thấy khái niệm này, chúng tôi hỏi thầy mục đích của nó là gì. Thầy nói, về cơ bản, từ dữ liệu cho trước, chúng ta cần tìm một hàm số để biến các các điểm đầu vào thành các điểm đầu ra tương ứng, không cần chính xác, chỉ cần xấp xỉ thôi.\n\nLúc đó, vốn là một học sinh chuyên toán, làm việc nhiều với đa thức ngày cấp ba, tôi đã quá tự tin trả lời ngay rằng Đa thức Nội suy Lagrange có thể làm được điều đó, miễn là các điểm đầu vào khác nhau đôi một! Thầy nói rằng “những gì ta biết chỉ là nhỏ xíu so với những gì ta chưa biết”. Và đó là những gì tôi muốn bắt đầu trong bài viết này.\n\nNhắc lại một chút về Đa thức nội suy Lagrange: Với \nN\n cặp điểm dữ liệu \n(\nx\n1\n,\ny\n1\n)\n,\n(\nx\n2\n,\ny\n2\n)\n,\n…\n,\n(\nx\nN\n,\ny\nN\n)\n với các \nx\ni\n kháu nhau đôi một, luôn tìm được một đa thức \nP\n(\n.\n)\n bậc không vượt quá \nN\n−\n1\n sao cho \nP\n(\nx\ni\n)\n=\ny\ni\n,\n \n∀\ni\n=\n1\n,\n2\n,\n…\n,\nN\n. Chẳng phải điều này giống với việc ta đi tìm một mô hình phù hợp (fit) với dữ liệu trong bài toán Supervised Learning hay sao? Thậm chí điều này còn tốt hơn vì trong Supervised Learning ta chỉ cần xấp xỉ thôi.\n\nSự thật là nếu một mô hình quá fit với dữ liệu thì nó sẽ gây phản tác dụng! Hiện tượng quá fit này trong Machine Learning được gọi là overfitting, là điều mà khi xây dựng mô hình, chúng ta luôn cần tránh. Để có cái nhìn đầu tiên về overfitting, chúng ta cùng xem Hình dưới đây. Có 50 điểm dữ liệu được tạo bằng một đa thức bậc ba cộng thêm nhiễu. Tập dữ liệu này được chia làm hai, 30 điểm dữ liệu màu đỏ cho training data, 20 điểm dữ liệu màu vàng cho test data. Đồ thị của đa thức bậc ba này được cho bởi đường màu xanh lục. Bài toán của chúng ta là giả sử ta không biết mô hình ban đầu mà chỉ biết các điểm dữ liệu, hãy tìm một mô hình “tốt” để mô tả dữ liệu đã cho.\n\n\t\n\t\nUnderfitting và Overfitting với Polynomial Regression (Source code).\nVới những gì chúng ta đã biết từ bài Linear Regression, với loại dữ liệu này, chúng ta có thể áp dụng Polynomial Regression. Bài toán này hoàn toàn có thể được giải quyết bằng Linear Regression với dữ liệu mở rộng cho một cặp điểm \n(\nx\n,\ny\n)\n là \n(\nx\n,\ny\n)\n với \nx\n=\n[\n1\n,\nx\n,\nx\n2\n,\nx\n3\n,\n…\n,\nx\nd\n]\nT\n cho đa thức bậc \nd\n. Điều quan trọng là chúng ta cần tìm bậc \nd\n của đa thức cần tìm.\n\nRõ ràng là một đa thức bậc không vượt quá 29 có thể fit được hoàn toàn với 30 điểm trong training data. Chúng ta cùng xét vài giá trị \nd\n=\n2\n,\n4\n,\n8\n,\n16\n. Với \nd\n=\n2\n, mô hình không thực sự tốt vì mô hình dự đoán quá khác so với mô hình thực. Trong trường hợp này, ta nói mô hình bị underfitting. Với \nd\n=\n8\n, với các điểm dữ liệu trong khoảng của training data, mô hình dự đoán và mô hình thực là khá giống nhau. Tuy nhiên, về phía phải, đa thức bậc 8 cho kết quả hoàn toàn ngược với xu hướng của dữ liệu. Điều tương tự xảy ra trong trường hợp \nd\n=\n16\n. Đa thức bậc 16 này quá fit dữ liệu trong khoảng đang xét, và quá fit, tức không được mượt trong khoảng dữ liệu training. Việc quá fit trong trường hợp bậc 16 không tốt vì mô hình đang cố gắng mô tả nhiễu hơn là dữ liệu. Hai trường hợp đa thức bậc cao này được gọi là Overfitting.\n\nNếu bạn nào biết về Đa thức nội suy Lagrange thì có thể hiểu được hiện tượng sai số lớn với các điểm nằm ngoài khoảng của các điểm đã cho. Đó chính là lý do phương pháp đó có từ “nội suy”, với các trường hợp “ngoại suy”, kết quả thường không chính xác.\n\nVới \nd\n=\n4\n, ta được mô hình dự đoán khá giống với mô hình thực. Hệ số bậc cao nhất tìm được rất gần với 0 (xem kết quả trong source code), vì vậy đa thức bậc 4 này khá gần với đa thức bậc 3 ban đầu. Đây chính là một mô hình tốt.\n\nOverfitting là hiện tượng mô hình tìm được quá khớp với dữ liệu training. Việc quá khớp này có thể dẫn đến việc dự đoán nhầm nhiễu, và chất lượng mô hình không còn tốt trên dữ liệu test nữa. Dữ liệu test được giả sử là không được biết trước, và không được sử dụng để xây dựng các mô hình Machine Learning.\n\nVề cơ bản, overfitting xảy ra khi mô hình quá phức tạp để mô phỏng training data. Điều này đặc biệt xảy ra khi lượng dữ liệu training quá nhỏ trong khi độ phức tạp của mô hình quá cao. Trong ví dụ trên đây, độ phức tạp của mô hình có thể được coi là bậc của đa thức cần tìm. Trong Multi-layer Perceptron, độ phức tạp của mô hình có thể được coi là số lượng hidden layers và số lượng units trong các hidden layers.\n\nVậy, có những kỹ thuật nào giúp tránh Overfitting?\n\nTrước hết, chúng ta cần một vài đại lượng để đánh giá chất lượng của mô hình trên training data và test data. Dưới đây là hai đại lượng đơn giản, với giả sử \ny\n là đầu ra thực sự (có thể là vector), và \n^\ny\n là đầu ra dự đoán bởi mô hình:\n\nTrain error: Thường là hàm mất mát áp dụng lên training data. Hàm mất mát này cần có một thừa số \n1\nN\ntrain\n để tính giá trị trung bình, tức mất mát trung bình trên mỗi điểm dữ liệu. Với Regression, đại lượng này thường được định nghĩa:\ntrain error\n=\n1\nN\ntrain\n∑\ntraining set\n \n∥\ny\n−\n^\ny\n∥\n2\np\nvới \np\n thường bằng 1 hoặc 2.\n\nVới Classification, trung bình cộng của cross entropy có thể được sử dụng.\n\nTest error: Tương tự như trên nhưng áp dụng mô hình tìm được vào test data. Chú ý rằng, khi xây dựng mô hình, ta không được sử dụng thông tin trong tập dữ liệu test. Dữ liệu test chỉ được dùng để đánh giá mô hình. Với Regression, đại lượng này thường được định nghĩa:\ntest error\n=\n1\nN\ntest\n∑\ntest set\n \n∥\ny\n−\n^\ny\n∥\n2\np\nvới \np\n giống như \np\n trong cách tính train error phía trên.\n\nViệc lấy trung bình là quan trọng vì lượng dữ liệu trong hai tập hợp training và test có thể chênh lệch rất nhiều.\n\nMột mô hình được coi là tốt (fit) nếu cả train error và test error đều thấp. Nếu train error thấp nhưng test error cao, ta nói mô hình bị overfitting. Nếu train error cao và test error cao, ta nói mô hình bị underfitting. Nếu train error cao nhưng test error thấp, tôi không biết tên của mô hình này, vì cực kỳ may mắn thì hiện tượng này mới xảy ra, hoặc có chỉ khi tập dữ liệu test quá nhỏ.\n\nChúng ta cùng đi vào phương pháp đầu tiên\n\n\n\n2. Validation\n\n\n2.1. Validation\nChúng ta vẫn quen với việc chia tập dữ liệu ra thành hai tập nhỏ: training data và test data. Và một điều tôi vẫn muốn nhắc lại là khi xây dựng mô hình, ta không được sử dụng test data. Vậy làm cách nào để biết được chất lượng của mô hình với unseen data (tức dữ liệu chưa nhìn thấy bao giờ)?\n\nPhương pháp đơn giản nhất là trích từ tập training data ra một tập con nhỏ và thực hiện việc đánh giá mô hình trên tập con nhỏ này. Tập con nhỏ được trích ra từ training set này được gọi là validation set. Lúc này, training set là phần còn lại của training set ban đầu. Train error được tính trên training set mới này, và có một khái niệm nữa được định nghĩa tương tự như trên validation error, tức error được tính trên tập validation.\n\nViệc này giống như khi bạn ôn thi. Giả sử bạn không biết đề thi như thế nào nhưng có 10 bộ đề thi từ các năm trước. Để xem trình độ của mình trước khi thi thế nào, có một cách là bỏ riêng một bộ đề ra, không ôn tập gì. Việc ôn tập sẽ được thực hiện dựa trên 9 bộ còn lại. Sau khi ôn tập xong, bạn bỏ bộ đề đã để riêng ra làm thử và kiểm tra kết quả, như thế mới “khách quan”, mới giống như thi thật. 10 bộ đề ở các năm trước là “toàn bộ” training set bạn có. Để tránh việc học lệch, học tủ theo chỉ 10 bộ, bạn tách 9 bộ ra làm training set thật, bộ còn lại là validation test. Khi làm như thế thì mới đánh giá được việc bạn học đã tốt thật hay chưa, hay chỉ là học tủ. Vì vậy, Overfitting còn có thể so sánh với việc Học tủ của con người.\n\nVới khái niệm mới này, ta tìm mô hình sao cho cả train error và validation error đều nhỏ, qua đó có thể dự đoán được rằng test error cũng nhỏ. Phương pháp thường được sử dụng là sử dụng nhiều mô hình khác nhau. Mô hình nào cho validation error nhỏ nhất sẽ là mô hình tốt.\n\nThông thường, ta bắt đầu từ mô hình đơn giản, sau đó tăng dần độ phức tạp của mô hình. Tới khi nào validation error có chiều hướng tăng lên thì chọn mô hình ngay trước đó. Chú ý rằng mô hình càng phức tạp, train error có xu hướng càng nhỏ đi.\n\nHình dưới đây mô tả ví dụ phía trên với bậc của đa thức tăng từ 1 đến 8. Tập validation bao gồm 10 điểm được lấy ra từ tập training ban đầu.\n\n\nHình 2: Lựa chọn mô hình dựa trên validation (Source code).\nChúng ta hãy tạm chỉ xét hai đường màu lam và đỏ, tương ứng với train error và validation error. Khi bậc của đa thức tăng lên, train error có xu hướng giảm. Điều này dễ hiểu vì đa thức bậc càng cao, dữ liệu càng được fit. Quan sát đường màu đỏ, khi bậc của đa thức là 3 hoặc 4 thì validation error thấp, sau đó tăng dần lên. Dựa vào validation error, ta có thể xác định được bậc cần chọn là 3 hoặc 4. Quan sát tiếp đường màu lục, tương ứng với test error, thật là trùng hợp, với bậc bằng 3 hoặc 4, test error cũng đạt giá trị nhỏ nhất, sau đó tăng dần lên. Vậy cách làm này ở đây đã tỏ ra hiệu quả.\n\nViệc không sử dụng test data khi lựa chọn mô hình ở trên nhưng vẫn có được kết quả khả quan vì ta giả sử rằng validation data và test data có chung một đặc điểm nào đó. Và khi cả hai đều là unseen data, error trên hai tập này sẽ tương đối giống nhau.\n\nNhắc lại rằng, khi bậc nhỏ (bằng 1 hoặc 2), cả ba error đều cao, ta nói mô hình bị underfitting.\n\n\n\n2.2. Cross-validation\nTrong nhiều trường hợp, chúng ta có rất hạn chế số lượng dữ liệu để xây dựng mô hình. Nếu lấy quá nhiều dữ liệu trong tập training ra làm dữ liệu validation, phần dữ liệu còn lại của tập training là không đủ để xây dựng mô hình. Lúc này, tập validation phải thật nhỏ để giữ được lượng dữ liệu cho training đủ lớn. Tuy nhiên, một vấn đề khác nảy sinh. Khi tập validation quá nhỏ, hiện tượng overfitting lại có thể xảy ra với tập training còn lại. Có giải pháp nào cho tình huống này không?\n\nCâu trả lời là cross-validation.\n\nCross validation là một cải tiến của validation với lượng dữ liệu trong tập validation là nhỏ nhưng chất lượng mô hình được đánh giá trên nhiều tập validation khác nhau. Một cách thường đường sử dụng là chia tập training ra \nk\n tập con không có phần tử chung, có kích thước gần bằng nhau. Tại mỗi lần kiểm thử , được gọi là run, một trong số \nk\n tập con được lấy ra làm validate set. Mô hình sẽ được xây dựng dựa vào hợp của \nk\n−\n1\n tập con còn lại. Mô hình cuối được xác định dựa trên trung bình của các train error và validation error. Cách làm này còn có tên gọi là k-fold cross validation.\n\nKhi \nk\n bằng với số lượng phần tử trong tập training ban đầu, tức mỗi tập con có đúng 1 phần tử, ta gọi kỹ thuật này là leave-one-out.\n\nSklearn hỗ trợ rất nhiều phương thức cho phân chia dữ liệu và tính toán scores của các mô hình. Bạn đọc có thể xem thêm tại Cross-validation: evaluating estimator performance.\n\n\n\n3. Regularization\nMột nhược điểm lớn của cross-validation là số lượng training runs tỉ lệ thuận với \nk\n. Điều đáng nói là mô hình polynomial như trên chỉ có một tham số cần xác định là bậc của đa thức. Trong các bài toán Machine Learning, lượng tham số cần xác định thường lớn hơn nhiều, và khoảng giá trị của mỗi tham số cũng rộng hơn nhiều, chưa kể đến việc có những tham số có thể là số thực. Như vậy, việc chỉ xây dựng một mô hình thôi cũng là đã rất phức tạp rồi. Có một cách giúp số mô hình cần huấn luyện giảm đi nhiều, thậm chí chỉ một mô hình. Cách này có tên gọi chung là regularization.\n\nRegularization, một cách cơ bản, là thay đổi mô hình một chút để tránh overfitting trong khi vẫn giữ được tính tổng quát của nó (tính tổng quát là tính mô tả được nhiều dữ liệu, trong cả tập training và test). Một cách cụ thể hơn, ta sẽ tìm cách di chuyển nghiệm của bài toán tối ưu hàm mất mát tới một điểm gần nó. Hướng di chuyển sẽ là hướng làm cho mô hình ít phức tạp hơn mặc dù giá trị của hàm mất mát có tăng lên một chút.\n\nMột kỹ thuật rất đơn giản là early stopping.\n\n\n\n3.1. Early Stopping\nTrong nhiều bài toán Machine Learning, chúng ta cần sử dụng các thuật toán lặp để tìm ra nghiệm, ví dụ như Gradient Descent. Nhìn chung, hàm mất mát giảm dần khi số vòng lặp tăng lên. Early stopping tức dừng thuật toán trước khi hàm mất mát đạt giá trị quá nhỏ, giúp tránh overfitting.\n\nVậy dừng khi nào là phù hợp?\n\nMột kỹ thuật thường được sử dụng là tách từ training set ra một tập validation set như trên. Sau một (hoặc một số, ví dụ 50) vòng lặp, ta tính cả train error và validation error, đến khi validation error có chiều hướng tăng lên thì dừng lại, và quay lại sử dụng mô hình tương ứng với điểm và validation error đạt giá trị nhỏ.\n\n\nHình 3: Early Stopping. Đường màu xanh là train error, đường màu đỏ là validation error. Trục x là số lượng vòng lặp, trục y là error. Mô hình được xác định tại vòng lặp mà validation error đạt giá trị nhỏ nhất. (Overfitting - Wikipedia).\nHình trên đây mô tả cách tìm điểm stopping. Chúng ta thấy rằng phương pháp này khá giống với phương pháp tìm bậc của đa thức ở phần trên của bài viết.\n\n\n\n3.2. Thêm số hạng vào hàm mất mát\nKỹ thuật regularization phổ biến nhất là thêm vào hàm mất mát một số hạng nữa. Số hạng này thường dùng để đánh giá độ phức tạp của mô hình. Số hạng này càng lớn, thì mô hình càng phức tạp. Hàm mất mát mới này thường được gọi là regularized loss function, thường được định nghĩa như sau:\nJ\nreg\n(\nθ\n)\n=\nJ\n(\nθ\n)\n+\nλ\nR\n(\nθ\n)\nNhắc lại rằng \nθ\n được dùng để ký hiệu các biến trong mô hình, chẳng hạn như các hệ số \nw\n trong Neural Networks. \nJ\n(\nθ\n)\n ký hiệu cho hàm mất mát (loss function) và \nR\n(\nθ\n)\n là số hạng regularization. \nλ\n thường là một số dương để cân bằng giữa hai đại lượng ở vế phải.\n\nViệc tối thiểu regularized loss function, nói một cách tương đối, đồng nghĩa với việc tối thiểu cả loss function và số hạng regularization. Tôi dùng cụm “nói một cách tương đối” vì nghiệm của bài toán tối ưu loss function và regularized loss function là khác nhau. Chúng ta vẫn mong muốn rằng sự khác nhau này là nhỏ, vì vậy tham số regularization (regularization parameter) \nλ\n thường được chọn là một số nhỏ để biểu thức regularization không làm giảm quá nhiều chất lượng của nghiệm.\n\nVới các mô hình Neural Networks, một số kỹ thuật regularization thường được sử dụng là:\n\n\n\n3.3. \nl\n2\n regularization\nTrong kỹ thuật này:\nR\n(\nw\n)\n=\n∥\nw\n∥\n2\n2\ntức norm 2 của hệ số.\n\nNếu bạn đọc chưa quen thuộc với khái niệm norm, bạn được khuyến khích đọc phần phụ lục này.\n\nHàm số này có một vài đặc điểm đang lưu ý:\n\nThứ nhất, \n∥\nw\n∥\n2\n2\n là một hàm số rất mượt, tức có đạo hàm tại mọi điểm, đạo hàm của nó đơn giản là \nw\n, vì vậy đạo hàm của regularized loss function cũng rất dễ tính, chúng ta có thể hoàn toàn dùng các phương pháp dựa trên gradient để cập nhật nghiệm. Cụ thể:\n∂\nJ\nreg\n∂\nw\n=\n∂\nJ\n∂\nw\n+\nλ\nw\nThứ hai, việc tối thiểu \n∥\nw\n∥\n2\n2\n đồng nghĩa với việc khiến cho các giá trị của hệ số \nw\n trở nên nhỏ gần với 0. Với Polynomial Regression, việc các hệ số này nhỏ có thể giúp các hệ số ứng với các số hạng bậc cao là nhỏ, giúp tránh overfitting. Với Multi-layer Perceptron, việc các hệ số này nhỏ giúp cho nhiều hệ số trong các ma trận trọng số là nhỏ. Điều này tương ứng với việc số lượng các hidden units hoạt động (khác không) là nhỏ, cũng giúp cho MLP tránh được hiện tượng overfitting.\nl\n2\n regularization là kỹ thuật được sử dụng nhiều nhất để giúp Neural Networks tránh được overfitting. Nó còn có tên gọi khác là weight decay. Decay có nghĩa là tiêu biến.\n\nTrong Xác suất thống kê, Linear Regression với \nl\n2\n regularization được gọi là Ridge Regression. Hàm mất mát của Ridge Regression có dạng:\nJ\n(\nw\n)\n=\n1\n2\n∥\ny\n−\nX\nw\n∥\n2\n2\n+\nλ\n∥\nw\n∥\n2\n2\ntrong đó, số hạng đầu tiên ở vế phải chính là hàm mất mát của Linear Regression. Số hạng thứ hai chính là phần regularization.\n\n\n\nVí dụ về Weight Decay với MLP\nChúng ta sử dụng mô hình MLP giống như bài trước nhưng dữ liệu có khác đi đôi chút.\n\n# To support both python 2 and python 3\nfrom __future__ import division, print_function, unicode_literals\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(4)\n\nmeans = [[-1, -1], [1, -1], [0, 1]]\ncov = [[1, 0], [0, 1]]\nN = 20\nX0 = np.random.multivariate_normal(means[0], cov, N)\nX1 = np.random.multivariate_normal(means[1], cov, N)\nX2 = np.random.multivariate_normal(means[2], cov, N)\nDữ liệu được tạo là ba cụm tuân theo phân phối chuẩn có tâm ở [[-1, -1], [1, -1], [0, 1]].\n\nTrong ví dụ này, chúng ta sử dụng số hạng regularization:\nλ\nR\n(\nW\n)\n=\nλ\nL\n∑\nl\n=\n1\n \n∥\nW\n(\nl\n)\n∥\n2\nF\nvới \n∥\n.\n∥\nF\n là Frobenius norm, là căn bậc hai của tổng bình phường các phẩn tử của ma trận.\n\n(Bạn đọc được khuyến khích đọc bài MLP để hiểu các ký hiệu).\n\nChú ý rằng weight decay ít khi được áp dụng lên biases. Tôi thay đổi tham số regularization \nλ\n và nhận được kết quả như sau:\n\n\t\n\t\nMulti-layer Perceptron với Weight Decay (Source code).\nKhi \nλ\n=\n0\n, tức không có regularization, ta nhận thấy gần như toàn bộ dữ liệu trong tập training được phân lớp đúng. Việc này khiến cho các class bị phân làm nhiều mảnh không được tự nhiên. Khi \nλ\n=\n0.001\n, vẫn là một số nhỏ, các đường phân chia trông tự nhiên hơn, nhưng lớp màu xanh lam vẫn bị chia làm hai bởi lớp màu xanh lục. Đây chính là biểu hiện của overfitting.\n\nKhi \nλ\n tăng lên, tức sự ảnh hưởng của regularization tăng lên (xem hàng dưới), đường ranh giới giữa các lớp trở lên tự nhiên hơn. Nói cách khác, với \nλ\n đủ lớn, weight decay có tác dụng hạn chế overfitting trong MLP.\n\nBạn đọc hãy thử vào trong Source code, thay \nλ\n=\n1\n bằng cách thay dòng cuối cùng:\n\nmynet(1)\nrồi chạy lại toàn bộ code, xem các đường phân lớp trông như thế nào. Gợi ý: underfitting.\n\nKhi \nλ\n quá lớn, tức ta xem phần regularization quan trọng hơn phần loss fucntion, một hiện tượng xấu xảy ra là các phần tử của \nw\n tiến về 0 để thỏa mãn regularization là nhỏ.\n\nSklearn có cung cấp rất nhiều chức năng cho MLP, trong đó ta có thể lựa chọn số lượng hidden layers và số lượng hidden units trong mỗi layer, activation functions, weight decay, learning rate, hệ số momentum, nesterovs_momentum, có early stopping hay không, lượng dữ liệu được tách ra làm validation set, và nhiều chức năng khác.\n\n\n\n3.4. Tikhonov regularization\nλ\nR\n(\nw\n)\n=\n∥\nΓ\nw\n∥\n2\n2\nVới \nΓ\n (viết hoa của gamma) là một ma trận. Ma trận \nΓ\n hay được dùng nhất là ma trận đường chéo. Nhận thấy rằng \nl\n2\n regularization chính là một trường hợp đặc biệt của Tikhonov regularization với \nΓ\n=\nλ\nI\n với \nI\n là ma trận đơn vị (the identity matrix), tức các phần tử trên đường chéo của \nΓ\n là như nhau.\n\nKhi các phần tử trên đường chéo của \nΓ\n là khác nhau, ta có một phiên bản gọi là weighted \nl\n2\n regularization, tức đánh trọng số khác nhau cho mỗi phần tử trong \nw\n. Phần tử nào càng bị đánh trọng số cao thì nghiệm tương ứng càng nhỏ (để đảm bảo rằng hàm mất mát là nhỏ). Với Polynomial Regression, các phần tử ứng với hệ số bậc cao sẽ được đánh trọng số cao hơn, khiến cho xác suất để chúng gần 0 là lớn hơn.\n\n\n\n3.5. Regularizers for sparsity\nTrong nhiều trường hợp, ta muốn các hệ số thực sự bằng 0 chứ không phải là nhỏ gần 0 như \nl\n2\n regularization đã làm phía trên. Lúc đó, có một regularization khác được sử dụng, đó là \nl\n0\n regularization:\nR\n(\nW\n)\n=\n∥\nw\n∥\n0\nNorm 0 không phải là một norm thực sự mà là giả norm. (Bạn được khuyến khích đọc thêm về norms (chuẩn)). Norm 0 của một vector là số các phần tử khác không của vector đó. Khi norm 0 nhỏ, tức rất nhiều phần tử trong vector đó bằng 0, ta nói vector đó là sparse.\n\nViệc giải bài toán tổi thiểu norm 0 nhìn chung là khó vì hàm số này không convex, không liên tục. Thay vào đó, norm 1 thường được sử dụng:\nR\n(\nW\n)\n=\n∥\nw\n∥\n1\n=\nd\n∑\ni\n=\n0\n \n|\nw\ni\n|\nNorm 1 là tổng các trị tuyệt đối của tất cả các phần tử. Người ta đã chứng minh được rằng tối thiểu norm 1 sẽ dẫn tới nghiệm có nhiều phần tử bằng 0. Ngoài ra, vì norm 1 là một norm thực sự (proper norm) nên hàm số này là convex, và hiển nhiên là liên tục, việc giải bài toán này dễ hơn việc giải bài toán tổi thiểu norm 0. Về \nl\n1\n regularization, bạn đọc có thể đọc thêm trong lecture note này. Việc giải bài toán \nl\n1\n regularization nằm ngoài mục đích của tôi trong bài viết này. Tôi hứa sẽ quay lại phần này sau. (Vì đây là phần chính trong nghiên cứu của tôi).\n\nTrong Thống Kê, việc sử dụng \nl\n1\n regularization còn được gọi là LASSO (Least Absolute Shrinkage and Selection Operator).\n\nKhi cả \nl\n2\n và \nl\n1\n regularization được sử dụng, ta có mô hình gọi là Elastic Net Regression.\n\n\n\n3.6. Regularization trong sklearn\nTrong sklearn, ví dụ Logistic Regression, bạn cũng có thể sử dụng các \nl\n1\n và \nl\n2\n regularizations bằng cách khai báo biến penalty='l1' hoặc penalty = 'l2' và biến C, trong đó C là nghịch đảo của \nλ\n. Trong các bài trước khi chưa nói về Overfitting và Regularization, tôi có sử dụng C = 1e5 để chỉ ra rằng \nλ\n là một số rất nhỏ.\n\n\n\n4. Các phương pháp khác\nNgoài các phương pháp đã nêu ở trên, với mỗi mô hình, nhiều phương pháp tránh overfitting khác cũng được sử dụng. Điển hình là Dropout trong Deep Neural Networks mới được đề xuất gần đây. Một cách ngắn gọn, dropout là một phương pháp tắt ngẫu nhiên các units trong Networks. Tắt tức cho các unit giá trị bằng không và tính toán feedforward và backpropagation bình thường trong khi training. Việc này không những giúp lượng tính toán giảm đi mà còn làm giảm việc overffitng. Tôi xin được quay lại vấn đề này nếu có dịp nói sâu về Deep Learning trong tương lai.\n\nBạn đọc có thể tìm đọc thêm với các từ khóa: pruning (tránh overfitting trong Decision Trees), VC dimension (đo độ phức tạp của mô hình, độ phức tạp càng lớn thì càng dễ bị overfitting).\n\n\n\n5. Tóm tắt nội dung\nMột mô hình mô tốt là mộ mô hình có tính tổng quát, tức mô tả được dữ liệu cả trong lẫn ngoài tập training. Mô hình chỉ mô tả tốt dữ liệu trong tập training được gọi là overfitting.\n\nĐể tránh overfitting, có rất nhiều kỹ thuật được sử dụng, điển hình là cross-validation và regularization. Trong Neural Networks, weight decay và dropout thường được dùng.\n\n\n\n6. Tài liệu tham khảo\n[1] Overfitting - Wikipedia\n\n[2] Cross-validation - Wikipedia\n\n[3] Pattern Recognition and Machine Learning\n\n[4] Krogh, Anders, and John A. Hertz. “A simple weight decay can improve generalization.” NIPS. Vol. 4. 1991.\n\n[5] Srivastava, Nitish, et al. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting” Journal of Machine Learning Research 15.1 (2014): 1929-1958.",
        "summary": "Overfitting là hiện tượng mô hình Machine Learning quá khớp với dữ liệu training, dẫn đến việc dự đoán sai nhiễu và chất lượng mô hình kém trên dữ liệu test. Để tránh overfitting, các kỹ thuật như cross-validation và regularization được sử dụng, trong đó cross-validation chia tập dữ liệu training thành nhiều tập con để đánh giá chất lượng mô hình trên nhiều tập validation khác nhau, còn regularization thêm một số hạng vào hàm mất mát để hạn chế độ phức tạp của mô hình. Một số kỹ thuật regularization phổ biến là weight decay, dropout, LASSO, và Elastic Net. \n",
        "status": true
    }
}